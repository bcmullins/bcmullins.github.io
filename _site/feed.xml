<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-27T23:04:46-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Brett Mullins</title><subtitle>Researcher - Data Scientist</subtitle><entry><title type="html">Economic Methodology Meets Interpretable Machine Learning - Part III - Responses to Friedman’s 1953 on the Realism of Assumptions</title><link href="http://localhost:4000/economic_methodology_interpretable_ml_samuelson_hausman/" rel="alternate" type="text/html" title="Economic Methodology Meets Interpretable Machine Learning - Part III - Responses to Friedman's 1953 on the Realism of Assumptions" /><published>2020-05-26T00:00:00-05:00</published><updated>2020-05-26T00:00:00-05:00</updated><id>http://localhost:4000/economic_methodology_interpretable_ml_samuelson_hausman</id><content type="html" xml:base="http://localhost:4000/economic_methodology_interpretable_ml_samuelson_hausman/">&lt;p&gt;This post discusses three responses to Friedman 1953 (which we introduced in &lt;a href=&quot;/economic_methodology_interpretable_ml_friedman1953/&quot;&gt;Part II&lt;/a&gt;). Friedman’s contention, termed the “F-Twist” by Samuelson, is that economic theories should be evaluated only on their predictions within some specified domain. The F-Twist puts Friedman on the instrumentalism end of the realism of assumptions debate. The responses by Paul Samuelson, Stanley Wong, and Dan Hausman discussed below provide various lenses though which to view the problem of the realism of assumptions and, ultimately, in my view, renders the F-Twist untenable in isolation.&lt;/p&gt;

&lt;h2 id=&quot;samuelsons-logic&quot;&gt;Samuelson’s Logic&lt;/h2&gt;

&lt;p&gt;Much like Milton Friedman, &lt;a href=&quot;https://en.wikipedia.org/wiki/Paul_Samuelson&quot;&gt;Paul Samuelson&lt;/a&gt; is one of the preeminent economists of the post-war period (though he is perhaps not as well known today). Samuelson and Friedman differed in their methodology just as they differed politically (Samuelson with a Keynesian bent and Friedman toward libertarianism). Samuelson’s critique of the F-Twist is spelled out in a &lt;a href=&quot;https://www.jstor.org/stable/i331698&quot;&gt;discussion paper&lt;/a&gt; from the 1963 American Economic Association meeting where he argues for a sort of realism about assumptions.&lt;/p&gt;

&lt;p&gt;Samuelson takes a deductivist position in his critique and, essentially, charges the F-Twist with &lt;a href=&quot;https://en.wikipedia.org/wiki/Affirming_the_consequent&quot;&gt;affirming the consequent&lt;/a&gt;. Let’s walk through Samuelson’s setup. He takes a model B to be a set of propositions in some formal language. Friedman’s analysis suggests that there exists a set C that are the consequences of B and a set A that is the assumptions of B. The assumptions A of the model B are logically equivalent to B but may be more succinctly expressed. Similarly, the consequences C of the model B is logically equivalent to B but is likely larger, since C contains B. On this view, A = B = C (at least up to deductive closure). Here is where Samuelson offers his interpretation of the F-Twist:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A theory is vindicable if (some of) its consequences are empirically valid to a useful degree of approximation; the (empirical) unrealism of the theory “itself,” or of its “assumptions,” is quite irrelevant to its validity and worth.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As Friedman points out, we’re often interested in predictions within a limited scope which we can call C-, a subset of C. Notice that C- is logically equivalent to some fragment of the model B, which we will call B-, and a fragment of the assumptions A, which we will call A-. Samuelson’s critique is that, on the one hand, the empirical adequacy of C- has no bearing on the empirical adequacy or reasonableness of A. On the other hand, the falsity of a proposition in A- implies that the propositions in C- are not necessarily true.&lt;/p&gt;

&lt;p&gt;So realism of assumptions does matter! Samuelson’s view acts as a realist counterpart to the F-Twist. It looks like this is a reasonable summary:&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;display: block;margin-left: auto; margin-right: auto; margin: 0px 0px 10px 10px&quot; width=&quot;600px&quot; height=&quot;250px&quot; src=&quot;/images/blog/econML/friedman_samuelson.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing to note here is that Friedman and Samuelson may have been talking past each other a bit. While Friedman’s setup seems to mesh better with econometric models and statistical analyses, it’s not clear how Samuelson’s deductive methodology works in practice in a stochastic world. Samuelson’s approach seems much more focused on formal reasoning.&lt;/p&gt;

&lt;h2 id=&quot;wongs-explanations&quot;&gt;Wong’s Explanations&lt;/h2&gt;

&lt;p&gt;Stanley Wong’s 1973 AER paper &lt;a href=&quot;https://www.jstor.org/stable/1914363?seq=1#metadata_info_tab_contents&quot;&gt;&lt;em&gt;The “F-Twist” and the Methodology of Paul Samuelson&lt;/em&gt;&lt;/a&gt; argues that Friedman vs Samuelson in methodology is a false dichotomy. There’s more on the menu than instrumentalism and descriptivism (what Wong calls Samuelson’s methodology). Moreover, Wong argues that both theories are inadequate because they lack an essential component of models: explanatory content.&lt;/p&gt;

&lt;p&gt;Here is how Wong closes his article:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;the descriptivist goal of logical equivalency denies a theory any explanatory content while the instrumentalist’s particular obsession with predictions disregards the explanatory content of a theory. The choice, then, is not between instrumentalism and descriptivism but between them both and the view that a theory is explanatory and informative, one which provides an answer, albeit a tentative one, to the question,”Why?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are two ways to read this quote on how the above graph is wrong. The first is that methodology is multidimensional and requires a second axis for explanatory content. While interesting, it’s not clear that we have to introduce this complexity. The second reading is that we should care about the realism of assumptions of a model inasmuch as they increase or enable the explanatory or informative content of the model.&lt;/p&gt;

&lt;h2 id=&quot;hausmans-extensions&quot;&gt;Hausman’s Extensions&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;https://hausman.philosophy.wisc.edu/papers/why-look-under-the-hood&quot;&gt;&lt;em&gt;Why Look Under the Hood?&lt;/em&gt;&lt;/a&gt;, Dan Hausman offers a simple and compelling argument for why we ought to care about the realism of assumptions. He offers us a reconstruction of the argument for the F-Twist:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;A good hypothesis provides valid and meaningful predictions concerning the class of phenomena it is intended to explain. (premise)&lt;/li&gt;
    &lt;li&gt;The only test of whether an hypothesis is a good hypothesis is whether it provides valid and meaningful predictions concerning the class of phenomena it is intended to explain. (invalidly from 1)&lt;/li&gt;
    &lt;li&gt;Any other facts about an hypothesis, including whether its assumptions are realistic, are irrelevant to its scientific assessment. (trivially from 2)&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, consider an argument of same form but regarding evaluating used cars rather than economic models:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;A good used car drives safely, economically and comfortably. (oversimplified premise)&lt;/li&gt;
    &lt;li&gt;The only test of whether a used car is a good used car is to check whether it drives safely, economically and comfortably. (invalidly from 1)&lt;/li&gt;
    &lt;li&gt;Anything one discovers by opening the hood and checking the separate components of a used car is irrelevant to its assessment. (trivially from 2)&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Just as there is something to be gained by looking under the hood of a used car, there is utility in inspecting the assumptions of models!&lt;/p&gt;

&lt;p&gt;Similar to how Wong argued that explanatory content is an essential part of a scientific or economic model, Hausman argues that models/theories are used to guide us in new circumstances:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Similarly, given Friedman’s view of the goal of science, there would be no point to examining the assumptions of a theory if it were possible to do a “total” assessment of its performance with respect to the phenomena it was designed to explain. But one cannot make such an assessment. Indeed the point of a theory is to guide us in circumstances where we do not already know whether the predictions are correct. There is thus much that may be learned by examining the components (assumptions) of a theory and its “irrelevant” predictions. Such consideration of the “realism” of assumptions is particularly important when extending the theory to new circumstances or when revising it in the face of predictive failure.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hausman’s take should be intuitive to machine learning practitioners such as when considering the behavior of a model a weird points in the feature space. Perhaps, for some application, we choose a tree-based model over a polynomial regression if the distribution of our data is wide and the regression is asymptotic in some directions.&lt;/p&gt;</content><author><name></name></author><summary type="html">This post discusses three responses to Friedman 1953 (which we introduced in Part II). Friedman’s contention, termed the “F-Twist” by Samuelson, is that economic theories should be evaluated only on their predictions within some specified domain. The F-Twist puts Friedman on the instrumentalism end of the realism of assumptions debate. The responses by Paul Samuelson, Stanley Wong, and Dan Hausman discussed below provide various lenses though which to view the problem of the realism of assumptions and, ultimately, in my view, renders the F-Twist untenable in isolation.</summary></entry><entry><title type="html">Economic Methodology Meets Interpretable Machine Learning - Part II - Friedman’s 1953</title><link href="http://localhost:4000/economic_methodology_interpretable_ml_friedman1953/" rel="alternate" type="text/html" title="Economic Methodology Meets Interpretable Machine Learning - Part II - Friedman's 1953" /><published>2020-05-02T00:00:00-05:00</published><updated>2020-05-02T00:00:00-05:00</updated><id>http://localhost:4000/economic_methodology_interpretable_ml_friedman1953</id><content type="html" xml:base="http://localhost:4000/economic_methodology_interpretable_ml_friedman1953/">&lt;p&gt;This post introduces &lt;a href=&quot;https://en.wikipedia.org/wiki/Milton_Friedman&quot;&gt;Milton Friedman’s&lt;/a&gt; 1953 essay &lt;em&gt;The Methodology of Positive Economics&lt;/em&gt; which takes the position that economic theories should be evaluated only on their predictions within some specified domain. This article &lt;a href=&quot;https://tint.helsinki.fi/publications/maki,%20readingF53.pdf&quot;&gt;has been called&lt;/a&gt; “the most cited, the most influential, and the most controversial piece of methodological writing in 20th century economics” and plays the foil (and occasionally the bogeyman) in much of the economic methodology literature. This is so much so that it is often referred to as Friedman 1953 or even F53.&lt;/p&gt;

&lt;p&gt;To illustrate this point, one influential commentator &lt;a href=&quot;https://afinetheorem.wordpress.com/2011/06/27/a-critique-of-friedmans-methodological-instrumentalism-b-caldwell-1980/&quot;&gt;sums up his view&lt;/a&gt; on Friedman’s methodology:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“if what you are doing looks like Popper or Friedman (1953), you are doing something gravely wrong. Friedman’s positive economics essay is so influential, though, that we need continue battening down the hatches as its destructive tempest swirls.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 0px&quot; width=&quot;300px&quot; height=&quot;300px&quot; src=&quot;/images/blog/econML/friedman_deductions.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For Friedman, economic theories consist of two parts: the model and the scope of the model. A model may be thought of as a set of propositions, and the scope is the range of instances that the model can be evaluated on for predictive accuracy. In this setup, predictions are just deductive inferences from the model. The scope component is necessary to combat the  position that assumptions of models ought to be as realistic as possible, since a model always deductively predicts/entails its assumptions. This leads to the &lt;em&gt;as if&lt;/em&gt; interpretation of economics where consumers and firms behave &lt;em&gt;as if&lt;/em&gt; they satisfy the assumptions of economic theory in some instances.&lt;/p&gt;

&lt;h2 id=&quot;motivating-example&quot;&gt;Motivating Example&lt;/h2&gt;

&lt;p&gt;Friedman offers the &lt;a href=&quot;http://noahpinionblog.blogspot.com/2016/06/the-pool-player-analogy-is-silly.html&quot;&gt;following example&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Consider the problem of predicting the shots made by an expert billiard player. It seems not at all unreasonable that excellent predictions would be yielded by the hypothesis that the billiard player made his shots as if he knew the complicated mathematical formulas that would give the optimum directions of travel, could estimate accurately by eye the angles, etc., describing the location of the balls, could make lightning calculations from the formulas, and could then make the balls travel in the direction indicated by the formulas. Our confidence in this hypothesis is not based on the belief that billiard players, even expert ones, can or do go through the process described; it derives rather from the belief that, unless in some way or other they were capable of reaching essentially the same result, they would not in fact be expert billiard players.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this example, we are attempting to model the behavior of expert billiard players. The model itself may be some sort of optimization solving for force and direction given the placement of balls on the table, and the scope of the model is limited to the force and direction of a given shot. Friedman argues that this optimization model for expert billiard players is good insofar as it accurately predicts shots which seems trivially (and maybe circularly) true in this instance. It’s not that the expert billiard player actually solves this optimization problem with each and every shot; rather, the billiard player acts &lt;em&gt;as if&lt;/em&gt; their behavior was determined by some solving a mathematical program. (As an aside, I wonder how much this example was influenced by the &lt;a href=&quot;https://mag.uchicago.edu/billiards&quot;&gt;Quadrangle Club&lt;/a&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;theory-choice&quot;&gt;Theory Choice&lt;/h2&gt;

&lt;p&gt;Friedman ventures into the philosophy of science with a wider discussion of &lt;a href=&quot;https://en.wikipedia.org/wiki/Theory_choice&quot;&gt;theory choice&lt;/a&gt;. On Friedman’s view, predictive accuracy within the model’s/theory’s scope is the primary criterion for choice between models/theories. This signals that his view is akin to instrumentalism about scientific theories. Rather than being concerned with the truth or falsity of scientific theories, instrumentalist views hold that theories that better perform according to some metric(s), e.g. predictive accuracy, should be preferred to those that do not. While predictive accuracy takes primacy, if two models/theories are similarly accurate with respect to a defined scope, then we may appeal to other theory virtues such as simplicity and so forth.&lt;/p&gt;

&lt;p&gt;A bit more daringly, Friedman also argues that “the more significant the theory, the more unrealistic the assumptions” (I believe he is heavily influenced by Karl Popper’s &lt;a href=&quot;https://plato.stanford.edu/entries/popper/#ProbKnowVeri&quot;&gt;&lt;em&gt;the more significant the theory the less likely it is to be true&lt;/em&gt;&lt;/a&gt; in saying this). If two theories are similarly accurate but the former has a wider scope than the latter, then the former will require a more complete description of reality, so its assumptions must be less accurate or more unrealistic. But we don’t need to worry about this dubious proposition here.&lt;/p&gt;

&lt;h2 id=&quot;in-practice&quot;&gt;In Practice&lt;/h2&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 10px 10px&quot; width=&quot;300px&quot; height=&quot;250px&quot; src=&quot;https://www.code4thought.eu/wp-content/uploads/2019/10/Picture1-300x210.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the one hand, Friedman’s methodology is not only difficult for the practitioner/economist to follow, but &lt;a href=&quot;https://medium.com/@UnlearningEcon/nobody-believes-friedman-1953-56ae462be599&quot;&gt;Friedman did not always follow it&lt;/a&gt;. On the other hand, Friedman’s economic methodology looks a lot like machine learning when viewed as an engineering task. Friedman seems to talk about economic theory the same way that data scientists and machine learning engineers talk about black-box models. Machine learning models are associative rules between variables that may or may not reflect actual relationships in the wild, and we restrict the scope of our models to distributions of data similar to the training data. (More on this analogy in future posts in this series.) In this way, Friedman seems to consider the practice of economics to be model building rather than explaining relationships between economic variables or isolating causal mechanisms (which both may just be a product of accidental historical trends).&lt;/p&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing Thoughts&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://afinetheorem.wordpress.com/2011/06/27/a-critique-of-friedmans-methodological-instrumentalism-b-caldwell-1980/&quot;&gt;Kevin Bryan summarizes&lt;/a&gt; Friedman’s view succinctly: “He thought that when theories say assumptions A implies consequence B, and A is wrong, we need to save the inference by claiming that we only care about B.”&lt;/p&gt;

&lt;p&gt;Friedman 1953 is the first essay in the collection &lt;a href=&quot;https://en.wikipedia.org/wiki/Essays_in_Positive_Economics&quot;&gt;&lt;em&gt;Essays in Positive Economics&lt;/em&gt;&lt;/a&gt; of the same year.&lt;/p&gt;</content><author><name></name></author><summary type="html">This post introduces Milton Friedman’s 1953 essay The Methodology of Positive Economics which takes the position that economic theories should be evaluated only on their predictions within some specified domain. This article has been called “the most cited, the most influential, and the most controversial piece of methodological writing in 20th century economics” and plays the foil (and occasionally the bogeyman) in much of the economic methodology literature. This is so much so that it is often referred to as Friedman 1953 or even F53.</summary></entry><entry><title type="html">Shadow on Pop!_OS/Ubuntu 19.10</title><link href="http://localhost:4000/shadow-on-popos-1910/" rel="alternate" type="text/html" title="Shadow on Pop!_OS/Ubuntu 19.10" /><published>2020-03-28T00:00:00-05:00</published><updated>2020-03-28T00:00:00-05:00</updated><id>http://localhost:4000/shadow-on-popos-1910</id><content type="html" xml:base="http://localhost:4000/shadow-on-popos-1910/">&lt;p&gt;I recently moved to a &lt;a href=&quot;https://system76.com&quot;&gt;System76&lt;/a&gt; Darter Pro running &lt;a href=&quot;https://system76.com/pop&quot;&gt;Pop!_OS 19.10&lt;/a&gt; as my primary laptop (review coming soon). As you might have guessed by the version number, Pop!_OS is System76’s fork of Ubuntu. With this move, I switched to &lt;a href=&quot;https://shadow.tech&quot;&gt;Shadow&lt;/a&gt; as my cloud gaming service, since they have a supported Linux client - no messing with Wine, dual boots, or VMs required! The Shadow Linux client is built to be compatible with 18.04+ but didn’t work right away due to some &lt;a href=&quot;https://wiki.archlinux.org/index.php/Hardware_video_acceleration&quot;&gt;Video Acceleration&lt;/a&gt; issues. Below is a guide for getting Shadow running on 19.10 based on my experience troubleshooting.&lt;/p&gt;

&lt;div style=&quot;content: ''; clear: both; display: flex;&quot;&gt;
  &lt;div style=&quot;float: left; width 40%; padding: 5px; flex: 40%&quot;&gt;
    &lt;img src=&quot;/images/blog/shadow_pop/pop.png&quot; /&gt;
  &lt;/div&gt;
  &lt;div style=&quot;float: right; width 60%; padding: 5px; flex:60%&quot;&gt;
    &lt;img src=&quot;/images/blog/shadow_pop/shadow_logo.jpeg&quot; /&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;a href=&quot;https://nicolasguilloux.github.io/blade-shadow-beta/&quot;&gt;Shadow Linux client&lt;/a&gt; is an easy to use AppImage file which looks to have been developed initially by the Shadow Linux community. Following the documentation for getting the VA-API to detect my onboard GPU, I ran into numerous issues with drivers. Despite my efforts, I could not get any results with the i965 driver, so I tried using the iHD driver suggested by a random StackExchange post. By switching drivers and passing a flag when launching Shadow, the client runs and streams without issues.&lt;/p&gt;

&lt;p&gt;Here are the steps:&lt;/p&gt;

&lt;p&gt;1 - Use &lt;code class=&quot;highlighter-rouge&quot;&gt;vainfo&lt;/code&gt; to find your GPU. If you see &lt;code class=&quot;highlighter-rouge&quot;&gt;-1&lt;/code&gt; returned anywhere (like below), then you likely have driver problems.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;vainfo
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; vainfo
libva info: VA-API version 1.5.0
libva info: va_getDriverName&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; returns 0
libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so
libva info: Found init &lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;__vaDriverInit_1_4
libva error: /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so init failed
libva info: va_openDriver&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; returns &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt;
vaInitialize failed with error code &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;unknown libva error&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,exit&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;2 - Install the &lt;a href=&quot;https://github.com/intel/media-driver&quot;&gt;drivers for the onboard GPU&lt;/a&gt;. Note that &lt;code class=&quot;highlighter-rouge&quot;&gt;intel-media-va-driver&lt;/code&gt; doesn’t seem to give you all that you need, so &lt;code class=&quot;highlighter-rouge&quot;&gt;intel-media-va-driver-non-free&lt;/code&gt; may be necessary.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;libva libmfx1 libmfx-tools intel-media-va-driver-non-free&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3 - Check that the drivers are recognized. You should be seeing both &lt;code class=&quot;highlighter-rouge&quot;&gt;H264&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;HEVC&lt;/code&gt; in the output.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;LIBVA_DRIVER_NAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iHD
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; vainfo
libva info: VA-API version 1.5.0
libva info: va_getDriverName&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; returns 0
libva info: User requested driver &lt;span class=&quot;s1&quot;&gt;'iHD'&lt;/span&gt;
libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so
libva info: Found init &lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;__vaDriverInit_1_5
libva info: va_openDriver&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; returns 0
vainfo: VA-API version: 1.5 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;libva 2.5.0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
vainfo: Driver version: Intel iHD driver - 1.0.0
vainfo: Supported profile and entrypoints
      VAProfileNone                   :	VAEntrypointVideoProc
      VAProfileNone                   :	VAEntrypointStats
      VAProfileMPEG2Simple            :	VAEntrypointVLD
      VAProfileMPEG2Simple            :	VAEntrypointEncSlice
      VAProfileMPEG2Main              :	VAEntrypointVLD
      VAProfileMPEG2Main              :	VAEntrypointEncSlice
      VAProfileH264Main               :	VAEntrypointVLD
      VAProfileH264Main               :	VAEntrypointEncSlice
      VAProfileH264Main               :	VAEntrypointFEI
      VAProfileH264Main               :	VAEntrypointEncSliceLP
      VAProfileH264High               :	VAEntrypointVLD
      VAProfileH264High               :	VAEntrypointEncSlice
      VAProfileH264High               :	VAEntrypointFEI
      VAProfileH264High               :	VAEntrypointEncSliceLP
      VAProfileVC1Simple              :	VAEntrypointVLD
      VAProfileVC1Main                :	VAEntrypointVLD
      VAProfileVC1Advanced            :	VAEntrypointVLD
      VAProfileJPEGBaseline           :	VAEntrypointVLD
      VAProfileJPEGBaseline           :	VAEntrypointEncPicture
      VAProfileH264ConstrainedBaseline:	VAEntrypointVLD
      VAProfileH264ConstrainedBaseline:	VAEntrypointEncSlice
      VAProfileH264ConstrainedBaseline:	VAEntrypointFEI
      VAProfileH264ConstrainedBaseline:	VAEntrypointEncSliceLP
      VAProfileVP8Version0_3          :	VAEntrypointVLD
      VAProfileVP8Version0_3          :	VAEntrypointEncSlice
      VAProfileHEVCMain               :	VAEntrypointVLD
      VAProfileHEVCMain               :	VAEntrypointEncSlice
      VAProfileHEVCMain               :	VAEntrypointFEI
      VAProfileHEVCMain10             :	VAEntrypointVLD
      VAProfileHEVCMain10             :	VAEntrypointEncSlice
      VAProfileVP9Profile0            :	VAEntrypointVLD
      VAProfileVP9Profile2            :	VAEntrypointVLD&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;4 - Test the video encoding. It’s a strange video… but it’s in the &lt;a href=&quot;https://github.com/Intel-Media-SDK/MediaSDK/wiki/Intel-media-stack-on-Ubuntu&quot;&gt;Intel Media Kit documentation&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;wget https://fate-suite.libav.org/h264-conformance/AUD_MW_E.264
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;/usr/share/mfx/samples/sample_decode h264 &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; AUD_MW_E.264 &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; 30 &lt;span class=&quot;nt&quot;&gt;-rgb4&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;5 - Run the Shadow AppImage with the &lt;em&gt;no sandbox&lt;/em&gt; option set.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;path/to/AppImage/directory
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;./Shadow.AppImage &lt;span class=&quot;nt&quot;&gt;--no-sandbox&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For the future, you may want to add the export statement from step three to your .bashrc file or create a shell script to that runs the export statement and the AppImage. Enjoy!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: I’m still not 100% confident in the details of how this method works under the hood. If anyone wants to chat about it, feel free to email me at brettcmullins at gmail.com.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">I recently moved to a System76 Darter Pro running Pop!_OS 19.10 as my primary laptop (review coming soon). As you might have guessed by the version number, Pop!_OS is System76’s fork of Ubuntu. With this move, I switched to Shadow as my cloud gaming service, since they have a supported Linux client - no messing with Wine, dual boots, or VMs required! The Shadow Linux client is built to be compatible with 18.04+ but didn’t work right away due to some Video Acceleration issues. Below is a guide for getting Shadow running on 19.10 based on my experience troubleshooting.</summary></entry><entry><title type="html">Some Fun with Knights and Knaves</title><link href="http://localhost:4000/knights-knaves-smullyan-part1/" rel="alternate" type="text/html" title="Some Fun with Knights and Knaves" /><published>2020-03-14T00:00:00-05:00</published><updated>2020-03-14T00:00:00-05:00</updated><id>http://localhost:4000/knights-knaves-smullyan-part1</id><content type="html" xml:base="http://localhost:4000/knights-knaves-smullyan-part1/">&lt;p&gt;I’m currently working through Raymond Smullyan’s &lt;a href=&quot;https://store.doverpublications.com/0486497054.html&quot;&gt;The Gödelian Puzzle Book&lt;/a&gt; and came across a fun problem that serves as a good starting point for new readers of Smullyan. Smullyan is well known for (among many other things) producing &lt;a href=&quot;https://en.wikipedia.org/wiki/Raymond_Smullyan#Logic_puzzles&quot;&gt;several books of logic puzzles&lt;/a&gt; that introduce ideas from mathematical and philosophical logic in an accessible but still technical way. These books are often formatted where each chapter has an introduction to the relevant characters, ideas, and setting, several problems for the reader to work through, and the solutions to the problems.
&lt;img style=&quot;float: right; display: inline-block; margin: 10px 0px 0px 20px; width: 200px; height: 300px&quot; src=&quot;https://sep.yimg.com/ca/I/yhst-137970348157658_2614_646619657&quot; /&gt;
I’ve found that working through Smullyan’s books helps build mathematical intuition better than repeatedly walking through proofs and applications of theorems. I would much rather go through Smullyan’s Gödel book than ever see &lt;a href=&quot;https://www.logicmatters.net/tyl/booknotes/enderton/&quot;&gt;Enderton’s Chapter 3&lt;/a&gt; again!&lt;/p&gt;

&lt;p&gt;The Knights and Knaves are a trope found throughout several of Smullyan’s books. Knights can only tell truths, while Knaves can only tell falsehoods. In Chapter 2 of The Gödelian Puzzle Book, our narrator finds himself on &lt;em&gt;The Island of Knights and Knaves&lt;/em&gt; where each inhabitant is either a Knight or a Knave. What follows is a fun, open-ended problem using natural language to reason about self-reference.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Problem 7.&lt;/strong&gt; One day I saw an extremely beautiful lady on this island and was immediately smitten with her. I longed to know whether or not she was married, but I did not have the courage to ask her. The next day I came across her two brothers Alfred and Bradford. Alfred then made a statement. From this statement I could not tell whether or not the lady was married. Then to my surprise, Bradford made the same statement, from which I could tell that she was not married. What statement could that have been?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Be sure to try to solve the problem before expanding the solution below!&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Solution&lt;/b&gt; (click the arrow to expand)&lt;/summary&gt;

  &lt;p&gt;There are several such solutions, but the one I landed on is &quot;I am a knave, my brother is a knave, and my sister is married.&quot; Let's refer to the two brothers as Brother A and Brother B. Let Brother A utter &quot;I am a knave, my brother is a knave, and my sister is married.&quot; Brother A can either be a knight or a knave. If Brother A is a knight, then he truly asserts that he is a knave: a contradiction; hence, Brother A is a knave. Then his utterance is false. So we have that either Brother A is a knight (we know to be false), Brother B is a knight, or the sister is not married. While we have some information, we do not yet know whether or not the sister is married.&lt;/p&gt;

  &lt;p&gt;Suppose now that Brother B utters &quot;I am a knave, my brother is a knave, and my sister is married.&quot; As with Brother A, Brother B must also be a knave. Since either Brother B is a knight or the sister is not married, it must be the case that the sister is unmarried!&lt;/p&gt;
&lt;/details&gt;

&lt;p&gt;I find that these open-ended problems are the most entertaining and instructive. Rather than asking the reader to determine $P$ or $\neg P$, these problems challenge the reader to create a sentence, situation, or condition. Being able to identify and find what is needed to solve some problem and then prove that it, indeed, solves the problem is much closer to how mathematics is actually practiced than the former approach.&lt;/p&gt;

&lt;p&gt;If you’re interested in more on Smullyan, I recommend picking up any of his puzzle books or &lt;a href=&quot;https://store.doverpublications.com/048649067x.html&quot;&gt;Four Lives&lt;/a&gt;, part biography of Smullyan, part collection of interesting mathematical and metamathematical thoughts, and part sampler of Smullyan’s most well known logic puzzles.&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m currently working through Raymond Smullyan’s The Gödelian Puzzle Book and came across a fun problem that serves as a good starting point for new readers of Smullyan. Smullyan is well known for (among many other things) producing several books of logic puzzles that introduce ideas from mathematical and philosophical logic in an accessible but still technical way. These books are often formatted where each chapter has an introduction to the relevant characters, ideas, and setting, several problems for the reader to work through, and the solutions to the problems. I’ve found that working through Smullyan’s books helps build mathematical intuition better than repeatedly walking through proofs and applications of theorems. I would much rather go through Smullyan’s Gödel book than ever see Enderton’s Chapter 3 again!</summary></entry><entry><title type="html">Economic Methodology Meets Interpretable Machine Learning - Part I - Interpretability, Explainability, and Black Boxes</title><link href="http://localhost:4000/economic_methodology_interpretable_ml_blackboxes/" rel="alternate" type="text/html" title="Economic Methodology Meets Interpretable Machine Learning - Part I - Interpretability, Explainability, and Black Boxes" /><published>2020-02-19T00:00:00-06:00</published><updated>2020-02-19T00:00:00-06:00</updated><id>http://localhost:4000/economic_methodology_interpretable_ml_blackboxes</id><content type="html" xml:base="http://localhost:4000/economic_methodology_interpretable_ml_blackboxes/">&lt;p&gt;This post is the first entry in Economic Methodology Meets Interpretable Machine Learning and briefly introduces the ideas of black boxes, explainability, and interpretability for machine learning models and offers arguments for and against deploying only interpretable models in the wild when interpretable models are available. The debate over interpretable models in machine learning is far from settled and has been getting &lt;a href=&quot;http://interpretable.ml/#Debate&quot;&gt;much attention in recent years&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is no clear consensus on what exactly black boxes, explainability, and interpretability mean as evidenced by (and despite) the dozens of review articles (that are hopefully not all saying the same thing). See &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://doi.acm.org/10.1145/3236009&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://ui.adsabs.harvard.edu/abs/2017arXiv170208608D&quot;&gt;here&lt;/a&gt; for a few. Take my approach as one perspective among many.&lt;/p&gt;

&lt;h2 id=&quot;black-boxes&quot;&gt;Black Boxes&lt;/h2&gt;

&lt;p&gt;Black box models often refer to one of two ideas. The first references models that are opaque to human inspection - think random forests with thousands of trees or neural networks with millions of weights. On this usage, opaque black boxes play the foil to transparent interpretable models as opposing ends on the translucency spectrum.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; src=&quot;/images/blog/econML/blackbox.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the first usage is usually taken to be a pejorative, the second refers not to the contents of the model itself but to the epistemic state of the developer/user with respect to the model. Putting the characterization in terms of the relative ignorance of the developer/user separates the context of the model’s construction from the model itself.&lt;/p&gt;

&lt;p&gt;For instance, we can imagine being handed a model (perhaps behind an API) without any information about how it was built except knowing its inputs/outputs and labeling such a model a black box. For all the developer/user knows, the model is arbitrarily complex. A good example of the second usage is with the &lt;a href=&quot;http://doi.acm.org/10.1145/2939672.2939778&quot;&gt;LIME paper&lt;/a&gt; which offers a method for “explaining the predictions of any classifier.”&lt;/p&gt;

&lt;p&gt;I find it most fruitful to use opaque for the first meaning and reserve black box for the second usage.&lt;/p&gt;

&lt;h2 id=&quot;explainability&quot;&gt;Explainability&lt;/h2&gt;

&lt;p&gt;Explainable models are an odd and interesting class of machine learning models. They encompass interpretable models as well as most (but not all) of the middle-ground between opaque and transparent. A model is explainable if it belongs to a class of models for which a &lt;em&gt;reliable&lt;/em&gt; explanation method exists.&lt;/p&gt;

&lt;p&gt;This begs the question of what even is an explanation! While some define explanations narrowly, I want to broadly define an explanation as a reason for a model’s behavior. This approach recognizes that there is a multiplicity of explanation types and methods for generating those explanations. For instance, the notion that I’m most interested in is local explanation which looks at the behavior of a model immediately around a point of interest in the feature space when explaining a model’s prediction at that point. My current work involves studying the &lt;a href=&quot;https://arxiv.org/abs/1910.08959&quot;&gt;local explanation of classifiers from a topological perspective&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;50%&quot; height=&quot;50%&quot; style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; src=&quot;/images/blog/econML/classifier_with_anchor.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While some explainability approaches try to work for all applications, e.g. &lt;a href=&quot;http://doi.acm.org/10.1145/2939672.2939778&quot;&gt;LIME&lt;/a&gt;, others are domain-specific such as methods for &lt;a href=&quot;https://medium.com/@sumup.ai/semantic-dna-to-analyze-messaging-effectiveness-an-application-of-explainable-nlp-3b6a1341bd2&quot;&gt;NLP&lt;/a&gt; and &lt;a href=&quot;https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf&quot;&gt;computer vision&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You may notice that we were vague about what it means to be a &lt;em&gt;reliable&lt;/em&gt; method above. This is because reliability is a fuzzy notion that comes in degrees. Broadly speaking, a method is reliable if it is able to often produce faithful explanations of model behavior within its intended domain.&lt;/p&gt;

&lt;p&gt;Finally, let’s note that not all models are explainable. Practitioners may have the intuition that all machine learning models are explainable, but this is likely because most model architectures commonly used in practice permit explainability methods (the jury is still out for complex neural networks, especially those with discontinuous activation functions). Here are two cases that will likely not allow for reliable explanation methods. Consider a single variable classifier that labels irrational numbers “A” and rational numbers “B”. For a more extreme example, let us consider a single variable classifier that assigns an algorithmically random set of real numbers “A” and all else “B”.&lt;/p&gt;

&lt;h2 id=&quot;interpretability&quot;&gt;Interpretability&lt;/h2&gt;

&lt;p&gt;Interpretable models are the counterpart to opaque models and are transparent on the translucency scale to continue the analogy above. Their architecture is sufficiently simple to allow the developer/user to reliably predict the model’s behavior with a reasonable amount of effort. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/simple.html&quot;&gt;Common interpretable models&lt;/a&gt; include decision trees, linear regression, and GLMs, though not all instances of these models are interpretable.&lt;/p&gt;

&lt;p&gt;Similar to explainability, interpretability is domain-specific with different approaches used for varying tasks, so there is no single unifying notion. For instance, see here for work on &lt;a href=&quot;https://arxiv.org/abs/1806.10574&quot;&gt;interpretable computer vision models&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Unlike explainability, however, interpretablility makes direct reference to the cognitive limitations/ability of humans. Interpretable models tend to have human-readable features (&lt;a href=&quot;https://medium.com/data-design/how-to-not-be-dumb-at-applying-principal-component-analysis-pca-6c14de5b3c9d&quot;&gt;so much for PCA&lt;/a&gt;) and a limited number of features overall, often fewer than 10 features.&lt;/p&gt;

&lt;p&gt;Given the transparency of interpretable models, explanations are built into their architecture so-to-speak and do not require a mediating explanation method. These explanations may take the form of comparing the signs and magnitude of coefficients in a linear regression or counterfactual paths through a decision tree. Explanations generated from interpretable models are always &lt;em&gt;reliable&lt;/em&gt; in the sense used above, excluding operator error.&lt;/p&gt;

&lt;h2 id=&quot;the-case-for-interpretability-in-the-wild&quot;&gt;The Case for Interpretability in the Wild&lt;/h2&gt;

&lt;p&gt;Recently, an argument has popped up in favor of using interpretable models rather than explanation methods when models are used in applications that inform decisions or affect users. While other arguments for interpretable models exist, I want to focus on this one in particular. Here’s what I take to be a convincing rendition of the argument:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Machine learning models are increasingly being employed to support high-stakes decisions in healthcare, the court system, and so forth. To trust a model to support these decisions in the wild, users and developers alike are interested in the reasons for a model’s prediction. Explainable models and interpretable models both offer such reasons: the former through domain-specific explanation methods, and the latter in virtue of its transparent structure. While reasons from interpretable models are justified &lt;em&gt;sui generis&lt;/em&gt;, what justifies the reasons provided by the particular explainability method chosen? Isn’t this just passing the buck from the model to the explainability method? This generates a justification regress of sorts (&lt;a href=&quot;/epistemic-justification/&quot;&gt;not uncommon to epistemologists&lt;/a&gt;) that overburdens both developers and users and that can be avoided by using interpretable models when interpretable models are available. Therefore, we should deploy only interpretable models in the wild when interpretable models are available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This argument is a strengthened version of the one given in &lt;a href=&quot;https://www.nature.com/articles/s42256-019-0048-x&quot;&gt;&lt;em&gt;Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead&lt;/em&gt;&lt;/a&gt; (That argument undersells explainability methods by narrowly defining explanations as global approximations and, in doing so, assumes what the author is trying to prove: explanations are at most as trustworthy than the models they’re trying to explain).&lt;/p&gt;

&lt;h2 id=&quot;the-case-against-interpretability-in-the-wild&quot;&gt;The Case Against Interpretability in the Wild&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;40%&quot; height=&quot;40%&quot; style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; src=&quot;/images/blog/econML/tradeoff.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One interesting argument posits that &lt;a href=&quot;https://link.springer.com/article/10.1007/s13347-019-00372-9&quot;&gt;interpretability is a means and not an end&lt;/a&gt;. The ends in question for machine learning models are fairness, privacy, stability, causality, and so forth. With the focus on interpretability, we may be (implicitly) assuming both that interpretability is a means at getting at these ends and, more strongly, that interpretability is the only means to get at these ends. At the very least, this should be suspect, since interpretability is domain-specific and lacks a unified definition.&lt;/p&gt;

&lt;p&gt;Here are some consequences of a potential undue focus on interpretability. Firstly, by constraining our model’s architecture to meet interpretability requirements, we are sacrificing some accuracy (though this amount is sometimes over-exaggerated as in the graphic to the right and sometimes under-exaggerated.) Secondly, legislating interpretability beyond a “best practice” in some contexts would conform to some interpretability notion(s) at the expense of others.&lt;/p&gt;</content><author><name></name></author><summary type="html">This post is the first entry in Economic Methodology Meets Interpretable Machine Learning and briefly introduces the ideas of black boxes, explainability, and interpretability for machine learning models and offers arguments for and against deploying only interpretable models in the wild when interpretable models are available. The debate over interpretable models in machine learning is far from settled and has been getting much attention in recent years.</summary></entry><entry><title type="html">Economic Methodology Meets Interpretable Machine Learning - Introduction</title><link href="http://localhost:4000/economic_methodology_interpretable_ml_intro/" rel="alternate" type="text/html" title="Economic Methodology Meets Interpretable Machine Learning - Introduction" /><published>2020-02-03T00:00:00-06:00</published><updated>2020-02-03T00:00:00-06:00</updated><id>http://localhost:4000/economic_methodology_interpretable_ml_intro</id><content type="html" xml:base="http://localhost:4000/economic_methodology_interpretable_ml_intro/">&lt;p&gt;In this series of posts, we will develop an analogy between the realistic assumptions debate in economic methodology and the current discussion over interpretability when using machine learning models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for machine learning theorists and practitioners to avoid analysis paralysis and make progress on the interpretability issue - one way or the other. But first, what’s going on with these two debates?&lt;/p&gt;

&lt;h3 id=&quot;interpretable-machine-learning&quot;&gt;Interpretable Machine Learning&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://christophm.github.io/interpretable-ml-book/images/title_page.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Machine learning models are increasingly prevalent in our lives from credit scores to autocomplete. Interpretable machine learning limits the types of predictive models available to the practitioner to those that allow humans to understand how the model inputs affect the model outputs. Put another way: interpretable machine learning models allow one to reliably predict the behavior of a model with a reasonable amount of effort. For instance, fixing the number of features, a linear classifier is more interpretable than a large random forest, since calculating a weighted sum is more transparent and simple than navigating hundreds of decision trees.&lt;/p&gt;

&lt;p&gt;There have been recent arguments for using only interpretable models in the wild when available. Among other reasons, since humans build and deploy these predictive models (in most cases at least), using interpretable methods allows the practitioner to better trust the accuracy of predictions across all possible inputs. While this argument has its merits, there are many grades of translucency between the opaque black box and the crystal-clear interpretable model in the form of explainable models.&lt;/p&gt;

&lt;p&gt;This debate asks where to draw the line of permissible translucency. Should we restrict ourselves to a small set of highly interpretable models when predictions are used for important decisions? If we take human understandability out of the equation, how reliable and simple should an explanation of a prediction be?&lt;/p&gt;

&lt;h3 id=&quot;realistic-assumptions-for-economic-models&quot;&gt;Realistic Assumptions for Economic Models&lt;/h3&gt;

&lt;p&gt;Economic models are built to capture some social interaction from the dynamics of a marketplace to strategic behavior between two parties. In instances where there are competing models for the same phenomena, it’s not immediately clear which of the models we should prefer and which to reject. One historically important position advanced by Milton Friedman in the 1950s holds that the veracity of an economic model is not relevant to its evaluation; only the predictive success of a model within a specified scope is relevant, a variant of a position called instrumentalism in the philosophy of science. We can imagine the other extreme would be to insist that &lt;em&gt;good&lt;/em&gt; models should be descriptively accurate, a position called realism.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;40%&quot; height=&quot;30%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Supply-and-demand.svg/1024px-Supply-and-demand.svg.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are few economic methodologists today who hold these sorts of instrumentalism and realism. On the one hand, many have the intuition that economic models should capture the causal mechanism at work in the wild. Perhaps, this means being descriptively accurate for crucial assumptions of the economic model. On the other hand, simplifying assumptions are often included in models for a variety of reasons such as analytical tractability, so we don’t expect economic models to be always fully accurate. This debate asks where to draw the line between instrumentalism and realism, how to choose between a predictively accurate but unrealistic model and a descriptively realistic but inaccurate model, and so forth.&lt;/p&gt;

&lt;h3 id=&quot;plan-for-future-posts&quot;&gt;Plan for Future Posts&lt;/h3&gt;

&lt;p&gt;I plan to post a new entry in this series every two weeks or so. If you want keep up to date with this series, you can subscribe by email in the box at the top of this page. Here is a brief summary of each entry:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part I&lt;/strong&gt; gives an overview of black box, explainable, and interpretable machine learning models. We sketch a few arguments focusing on model trust against using black box models in the wild. While some argue for using explainability techniques such as LIME, others push for restricting practitioners to only deploying interpretable models when available and appropriate.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/en/3/32/EssaysInPositiveEconomics.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part II&lt;/strong&gt; introduces Milton Friedman’s 1953 article &lt;em&gt;The Methodology of Positive Economics&lt;/em&gt;. This article is regarded as “the most cited, influential, and controversial piece of methodological writing in twentieth-century economics”, arguing that the assumptions of an economic model are unimportant so long as the model is predictive within its domain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part III&lt;/strong&gt; looks at responses to Friedman by Paul Samuelson, Stanley Wong, and Dan Hausman which capture the some of the intuition of what’s problematic with Friedman’s argument.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part IV&lt;/strong&gt; considers the current state of the realistic assumptions debate and introduces two contemporary arguments: one in support of instrumentalism and another which holds that economists follow a middle position in practice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part V&lt;/strong&gt; ties together the analogy that’s been building during the first four entries. We’ll argue for two conclusions for interpretable machine learning: one practical and the other theoretical.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part VI&lt;/strong&gt; considers some objections and pain points where the analogy breaks down. For instance, we will look at how differences in the aims of economics and machine learning affect our conclusions.&lt;/p&gt;</content><author><name></name></author><summary type="html">In this series of posts, we will develop an analogy between the realistic assumptions debate in economic methodology and the current discussion over interpretability when using machine learning models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for machine learning theorists and practitioners to avoid analysis paralysis and make progress on the interpretability issue - one way or the other. But first, what’s going on with these two debates?</summary></entry><entry><title type="html">Top Books I’ve Read in 2019</title><link href="http://localhost:4000/Top-Books-2019/" rel="alternate" type="text/html" title="Top Books I've Read in 2019" /><published>2019-12-18T00:00:00-06:00</published><updated>2019-12-18T00:00:00-06:00</updated><id>http://localhost:4000/Top-Books-2019</id><content type="html" xml:base="http://localhost:4000/Top-Books-2019/">&lt;p&gt;Here are the top three books I’ve read in 2019, presented below in chronological order by year published. While quite the cliché, the theme that emerged this year is to not judge a book by its cover. While Measure and Category by John Oxtoby appears to be a terse math treatise, it is a short, well-paced, lucid read (though requiring &lt;a href=&quot;/measure_theory_resources/&quot;&gt;some prerequisites&lt;/a&gt;). Braudel’s The Structures of Everyday Life digs deeply into the minutiae of common experience in early modern Europe rather than providing overarching historical narrative. To finish, Haskel and Weslake’s Capitalism without Capital is a well-researched - if at times dull - look at intangible assets from an economic perspective whose title reminds one of a political polemic.&lt;/p&gt;

&lt;h2 id=&quot;measure-and-category-a-survey-of-the-analogies-between-topological-and-measure-spaces&quot;&gt;Measure and Category: A Survey of the Analogies between Topological and Measure Spaces&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: John C. Oxtoby&lt;/p&gt;

&lt;p&gt;Published: 1971 (1980 2nd ed.)&lt;/p&gt;

&lt;p&gt;Measure and Category is a slim mathematics monograph that explores connections between concepts in measure theory and topology. From this book’s cover, one would expect the typical, terse Springer(-Verlag) &lt;a href=&quot;https://www.springer.com/series/136&quot;&gt;Graduate Texts in Mathematics&lt;/a&gt; affair. To the contrary, a &lt;a href=&quot;https://mathoverflow.net/questions/11591/suggestions-for-a-good-measure-theory-book&quot;&gt;recommendation on Stack Exchange&lt;/a&gt; describes Measure and Category as “a fantastic little book. After you have studied the others, you can read through this like a novel and everything will start to fit together much more. Pure inspiration.”&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://images.springer.com/sgw/books/medium/9780387905082.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To give an example of what’s going on in this book, let’s compare two notions of &lt;em&gt;smallness&lt;/em&gt; applied to subsets of the real numbers. In measure theory, the (Lebesgue) measure is a generalization of the length of an interval, e.g. the measure of the unit interval is one and the measure of the real numbers is infinite. Collections of real numbers with zero measure, also called &lt;a href=&quot;https://en.wikipedia.org/wiki/Null_set&quot;&gt;null sets&lt;/a&gt;, can be thought of as &lt;em&gt;small&lt;/em&gt;. An alternative notion of &lt;em&gt;smallness&lt;/em&gt; from a topological perspective is being &lt;a href=&quot;https://en.wikipedia.org/wiki/Meagre_set&quot;&gt;meagre&lt;/a&gt;. Among other properties, these sets contain no open intervals as subsets. So meagre sets are &lt;em&gt;small&lt;/em&gt; because they lack thickness.&lt;/p&gt;

&lt;p&gt;To illustrate the similarities between null sets and meagre sets, any countable number of null (meagre) sets combined together is a null (meagre) set. So these sets are so small that combining them together with other small sets always yields another small set. Oxtoby provides several &lt;em&gt;duality&lt;/em&gt; results specifying conditions when null and meagre can be interchanged in a theorem. While these properties are similar, not every null set is meagre nor is every meagre set null. In fact, we can show that the real numbers &lt;a href=&quot;https://mathoverflow.net/questions/43478/is-there-a-measure-zero-set-which-isnt-meagre&quot;&gt;can be partitioned&lt;/a&gt; into a null set and a meagre set of relatively low complexity!&lt;/p&gt;

&lt;p&gt;I came across this book when working on a &lt;a href=&quot;https://arxiv.org/abs/1910.08595&quot;&gt;topological approach to explainable machine learning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-structures-of-everyday-life-civilization-and-capital-15th---18th-century-volume-1&quot;&gt;The Structures of Everyday Life: Civilization and Capital, 15th - 18th Century: Volume 1&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Fernand Braudel&lt;/p&gt;

&lt;p&gt;Published: 1981&lt;/p&gt;

&lt;p&gt;The first entry of Braudel’s Civilization and Capital trilogy paints a picture of everyday life in Europe during the 14th to the 18th centuries. Rather than focus on the lives and happenings of so-called “great men”, Braudel discusses the minutiae that is easily overlooked in common experience. These range from the evolution of food and drink to fashion, housing, and decor, to manners, money, and technology.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://images.ucpress.edu/covers/300/9780520081147.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most fruitful way to understand this book is as a primer to a particular epoch that sets the stage for the next two volumes of the series. As the goal is to provide the reader with a vivid context, Braudel regularly ventures outside of Europe and into the past to tell the story of how a state of affairs played out and what others were doing around the same time. In this sense, this book is much more broad in scope than it initially claims.&lt;/p&gt;

&lt;p&gt;Braudel can be somewhat uneven as he jumps between topics. For instance, the book begins with a long (perhaps) 70-page digression on the statistics of world population/demography from the late Middle Ages to the 20th century. Similarly, it is apparent that Braudel is quite interested in the Chinese, providing the foil in countless comparisons with Europe.&lt;/p&gt;

&lt;p&gt;The author does briefly venture into big questions asking why it is that Europeans and not the Chinese initiated the age of explorations, despite the head start of the Chinese. While these questions are surely interesting and worthwhile, this book shines in the details. I would be remiss not to mention that throughout there are numerous beautiful color illustrations of paintings and documents (for the hardback printing at least).&lt;/p&gt;

&lt;p&gt;This is a book where even if one takes nothing specific away, one will take away much context. The &lt;a href=&quot;https://www.binghamton.edu/fbc/&quot;&gt;Fernand Braudel Center at Binghamton University&lt;/a&gt; continues social science research in Braudel’s tradition.&lt;/p&gt;

&lt;h2 id=&quot;capitalism-without-capital&quot;&gt;Capitalism without Capital&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Jonathan Haskel and Stian Weslake&lt;/p&gt;

&lt;p&gt;Published: 2018&lt;/p&gt;

&lt;p&gt;With the title Capitalism without Capital, one may expect a polemic against neoliberalism or a cypherpunk manifesto. Instead, Haskel and Weslake provide a detailed initial investigation into how the increased prevalence of intangible goods affects both our economic statistics and the marketplace.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://pup-assets.imgix.net/onix/images/9780691175034.jpg?w=640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The book opens with a succinct example of the growth of intangible assets. Consider a gym from 1970 versus a gym today. By their nature, gyms are physical asset heavy ventures both then and now. The gym of today, however, may include many additional intangible assets, e.g., software organizing memberships, offering classes remotely through streaming video, etc. It is just these sort of intangible assets and goods that our economic statistics can miss!&lt;/p&gt;

&lt;p&gt;Outside of the accounting issues, the increased presence of intangible goods affects the marketplace, especially for firms that primarily produce intangible goods. The market for word processor software, for instance, is heavily dominated by one product since there isn’t enough differential need to warrant more competition: given the nature of the product, one word processor is good enough. On another front, given that business loans through bank financing often require either the firm’s physical assets or stakeholders’ assets as collateral, these firms will rely more heavily on VC and active investors that are in a position to better understand the potential value of the firm’s assets.&lt;/p&gt;

&lt;p&gt;This book is an interesting read despite being a bit dry from time to time. I certainly have a better appreciation for firms producing software and a better understanding of the role of VC in the software industry. I came across Capitalism without Capital from &lt;a href=&quot;https://www.gatesnotes.com/Books/Capitalism-Without-Capital&quot;&gt;Bill Gates’ recommendation&lt;/a&gt;, and I suspect that many others found it through this same means.&lt;/p&gt;</content><author><name></name></author><summary type="html">Here are the top three books I’ve read in 2019, presented below in chronological order by year published. While quite the cliché, the theme that emerged this year is to not judge a book by its cover. While Measure and Category by John Oxtoby appears to be a terse math treatise, it is a short, well-paced, lucid read (though requiring some prerequisites). Braudel’s The Structures of Everyday Life digs deeply into the minutiae of common experience in early modern Europe rather than providing overarching historical narrative. To finish, Haskel and Weslake’s Capitalism without Capital is a well-researched - if at times dull - look at intangible assets from an economic perspective whose title reminds one of a political polemic.</summary></entry><entry><title type="html">Top Articles I’ve Read in 2019</title><link href="http://localhost:4000/Top-Articles-2019/" rel="alternate" type="text/html" title="Top Articles I've Read in 2019" /><published>2019-12-12T00:00:00-06:00</published><updated>2019-12-12T00:00:00-06:00</updated><id>http://localhost:4000/Top-Articles-2019</id><content type="html" xml:base="http://localhost:4000/Top-Articles-2019/">&lt;p&gt;Below are the top eleven articles I’ve read in 2019. A theme of methodology runs through this set of papers, especially statistical methodology. There’s also some fun miscellany mixed in with blockchain (whose craze seems like a lifetime ago now), unicorns, and the history of the English language. To my surprise, all of these articles are from the present decade. They are presented in chronological order.&lt;/p&gt;

&lt;h2 id=&quot;the-credibility-revolution-in-empirical-economics-how-better-research-design-is-taking-the-con-out-of-econometrics&quot;&gt;The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con out of Econometrics&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Joshua D. Angrist and Jörn-Steffen Pischke&lt;/p&gt;

&lt;p&gt;Publication: Journal of Economic Perspectives&lt;/p&gt;

&lt;p&gt;Published: Spring 2010&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1257/jep.24.2.3&quot;&gt;10.1257/jep.24.2.3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper is in some sense a response to Ed Leamer’s 1983 &lt;a href=&quot;https://www.jstor.org/stable/1803924&quot;&gt;Let’s Take the Con Out of Econometrics&lt;/a&gt; that questioned the methods of econometrics both in theory and in practice for parading as objective science when in reality several hidden judgements are made by practitioners. Angrist and Pischke argue that, since Leamer’s critique, a focus on research design, i.e., the way in which studies are formulated and data is collected, has led to a revolution in econometrics.  Specifically, the use of quasi-experimental methods such as instrumental variables, regression discontinuity design, and difference-in-differences has led to more robust estimates of causal effects. Whereas Leamer proposed sensitivity analysis as a solution, the authors contend that better design better addresses Leamer’s worries.&lt;/p&gt;

&lt;figure style=&quot;float: right; display: inline-block&quot;&gt;
  &lt;img src=&quot;/images/blog/top_articles_2019/rdd.png&quot; width=&quot;350&quot; height=&quot;275&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://davidevans.blog/2018/05/18/activity-for-teaching-regression-discontinuity-design/&quot;&gt;Example Regression Discontinuity Design&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Angrist and Pischke give several examples with a focus on applied microeconomics, specifically labor and education, since these fields adapt easily to the quasi-experimental methodology. This paper sheds light on the current practice of econometrics, or at least how one influential party sees it. The authors remark that these methods may not be sexy or directly and quickly get at the “big” questions, but the quasi-experimental approach to econometrics proceeds by accumulation and small gains in understanding, similar to how we think about the hard sciences.&lt;/p&gt;

&lt;p&gt;This paper gives one a nice introduction to the ideas and context of Angrist and Pischke’s text &lt;a href=&quot;https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics&quot;&gt;Mostly Harmless Econometrics&lt;/a&gt; or their more friendly &lt;a href=&quot;https://www.masteringmetrics.com&quot;&gt;Mastering ‘Metrics&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;english-is-not-normal&quot;&gt;English is not normal&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: John McWhorter&lt;/p&gt;

&lt;p&gt;Publication: Aeon&lt;/p&gt;

&lt;p&gt;Published: Nov 2015&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://aeon.co/essays/why-is-english-so-weirdly-different-from-other-languages&quot;&gt;aeon.co/essays/why-is-english-so-weirdly-different-from-other-languages&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is a quirky essay about the history of the English language. The author argues its unique place among languages is largely an artifact of history. Here are two interesting tidbits. For the first, Old English is basically unrecognizable from contemporary English; witness the following: &lt;em&gt;“Hwæt, we gardena in geardagum þeodcyninga þrym gefrunon”&lt;/em&gt; is Old English for “So, we Spear-Danes have heard of the tribe-kings’ glory in days of yore”. While that sentence seems a bit contrived, you get the point. For the second tidbit, over time, English adopted French and Latin words along with a natural scale of refinement and formality: for example, kingly (English), royal (French), regal (Latin) all denote the same idea but express it with increasing regard.&lt;/p&gt;

&lt;p&gt;The author leaves us with this summary:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“The common idea that English dominates the world because it is ‘flexible’ implies that there have been languages that failed to catch on beyond their tribe because they were mysteriously rigid. I am not aware of any such languages. What English does have on other tongues is that it is deeply peculiar in the structural sense. And it became peculiar because of the slings and arrows – as well as caprices – of outrageous history.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-need-for-cognitive-science-in-methodology&quot;&gt;The Need for Cognitive Science in Methodology&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Sander Greenland&lt;/p&gt;

&lt;p&gt;Publication: American Journal of Epidemiology&lt;/p&gt;

&lt;p&gt;Published: Aug 2017&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1093/aje/kwx259&quot;&gt;10.1093/aje/kwx259&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There has been much recent literature on the deficiency of current statistical methods; however, not much attention has been paid to cognitive biases of practitioners using these methods. Greenland describes three such biases:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) dichotomania, the compulsion to perceive quantities as dichotomous even when dichotomization is unnecessary and misleading, as in inferences based on whether a P value is “statistically significant”; 2) nullism, the tendency to privilege the hypothesis of no difference or no effect when there is no scientific basis for doing so, as when testing only the null hypothesis; and 3) statistical reification, treating hypothetical data distributions and statistical models as if they reflect known physical laws rather than speculative assumptions for thought experiments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While these points have been brought up by &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&quot;&gt;Gelman&lt;/a&gt; and others in some form, Greenland provides a cohesive and convincing account of when it is that the tools themselves aren’t the (only) problem. He argues in favor of the methodological position that “Statistical analyses are merely thought experiments, informing us as to what would follow deductively under their assumptions” (&lt;a href=&quot;https://statmodeling.stat.columbia.edu/2019/12/03/whats-wrong-with-bayes/&quot;&gt;Gelman agrees&lt;/a&gt;). While an interesting philosophical position (and one have sympathies for), this view certainly doesn’t match my experience with how practitioners treat statistical analyses in the wild. Perhaps, that’s additional evidence that we ought to be thinking about the cognitive status of practitioners.&lt;/p&gt;

&lt;p&gt;Amusing quotes are peppered throughout: “This null obsession is the most destructive pseudoscientific gift that conventional statistics (both frequentist and Bayesian) has given the modern world.”&lt;/p&gt;

&lt;p&gt;Greenland also takes a swipe at &lt;a href=&quot;https://plato.stanford.edu/entries/learning-formal/&quot;&gt;formal methodology&lt;/a&gt; and &lt;a href=&quot;https://plato.stanford.edu/entries/pseudo-science/#KarPop&quot;&gt;philosophy of science&lt;/a&gt;: “thinking that parsimony is a property of nature when it is instead only an effective learning heuristic, or that refutationism involves believing hypotheses until they are falsified, when instead it involves never asserting a hypothesis is true”. While I buy much of what Greenland has to say, I may be too much of an ideologue to go this far.&lt;/p&gt;

&lt;h2 id=&quot;blockchain-is-not-only-crappy-technology-but-a-bad-vision-for-the-future&quot;&gt;Blockchain is not only crappy technology but a bad vision for the future&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Kai Stinchcombe&lt;/p&gt;

&lt;p&gt;Published: Apr 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec&quot;&gt;medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec&lt;/a&gt;&lt;/p&gt;

&lt;figure style=&quot;float: right; display: inline-block&quot;&gt;
  &lt;img src=&quot;/images/blog/top_articles_2019/blockchainmail.jpg&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center&quot;&gt;Blockchainmail&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This is a followup to &lt;a href=&quot;https://hackernoon.com/ten-years-in-nobody-has-come-up-with-a-use-case-for-blockchain-ee98c180100&quot;&gt;Ten years in, nobody has come up with a use for blockchain&lt;/a&gt;. The author argues two points: people often confuse blockchain the idea with blockchain the technology; and we actually do not want fully trustless transactions. On the first point, people want a way for correct information to be entered into a tamperproof database, e.g., tracking fruit to ensure it’s organic; however, nothing stops bad data from being entered into the database. On the second point, the author defends the value of social trust and points out examples where even major uses of cryptocurrencies, e.g., the Silk Road and Ripple, rely on trust measures, e.g., seller reviews, rather than smart contracts or anything else of the sort. These social institutions are better than having to trust the code of someone offering a smart contract or from having to write one’s contract every time one wishes to make a transaction. Trading this trust for other ends leads to a world that’s “not a paradise but a crypto-medieval hellhole.”&lt;/p&gt;

&lt;h2 id=&quot;why-data-is-never-raw&quot;&gt;Why Data is Never Raw&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Nick Barrowman&lt;/p&gt;

&lt;p&gt;Publication: The New Atlantis&lt;/p&gt;

&lt;p&gt;Published: Summer/Fall 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.thenewatlantis.com/publications/why-data-is-never-raw&quot;&gt;www.thenewatlantis.com/publications/why-data-is-never-raw&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article applies the idea of &lt;a href=&quot;https://plato.stanford.edu/entries/science-theory-observation/#HowObsEviMigTheLad&quot;&gt;theory-ladeness of observation&lt;/a&gt; from the philosophy of science to the idea of raw data in statistics and data science. Just as the presuppositions of an observer affect how one interprets what is observed, how data are captured (not just given) affect what the data say.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“For example, rates of domestic violence were historically underestimated because these crimes were rarely documented. Polling data may miss people who are homeless or institutionalized, and if marginalized people are incompletely represented by opinion polls, the results may be skewed. Data sets often preferentially include people who are more easily reached or more likely to respond.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For this reason, the method of data acquisition is highly relevant to its interpretation and use whether for survey data from such and such a population, measurements in a lab, or transaction logs from a production system. To sum up, the author offers the words of Geoffrey Bowker of UC Irvine: “Raw data is both an oxymoron and a bad idea; to the contrary, data should be cooked with care.”&lt;/p&gt;

&lt;h2 id=&quot;the-bias-bias-in-behavioral-economics&quot;&gt;The Bias Bias in Behavioral Economics&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Gerd Gigerenzer&lt;/p&gt;

&lt;p&gt;Publication: Review of Behavioral Economics&lt;/p&gt;

&lt;p&gt;Published: Dec 2018&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1561/105.00000092&quot;&gt;10.1561/105.00000092&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article is an attack on the current state of behavioral economics. Since the work of Kahneman and Tverksy in the 1970s, behavioral economics has largely focused on the identification of &lt;a href=&quot;https://hbr.org/2015/05/from-economic-man-to-behavioral-economics&quot;&gt;biases and heuristics for decision making&lt;/a&gt;. Biases are consistent and widespread departures from the rational choice model. The author argues for the bias bias: attributing a bias to behavior that is potentially well-motivated.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;40%&quot; height=&quot;50%&quot; src=&quot;/images/blog/top_articles_2019/coin_flip.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most interesting bias discussed is the law of small numbers: “People think a string is more likely the closer the number of heads and tails corresponds to the underlying equal probabilities. For instance, the string HHHHHT is deemed more likely than HHHHHH.” When we consider six coin flips, this intuition is clearly incorrect since each possible outcome is equally likely and the events are independent. Consider, instead, if there are seven coin flips. We still have that all outcomes are equally likely; however, HHHHHT appears as a substring in four outcomes, while HHHHHH only appears in three. This result holds for any number of coin flips greater than the length of the substring. This potentially explains people’s otherwise confused intuitions.&lt;/p&gt;

&lt;p&gt;The paper concludes by suggesting that behavioral economics abandon its current research program and follow the alternative &lt;a href=&quot;https://plato.stanford.edu/entries/bounded-rationality/&quot;&gt;bounded rationality&lt;/a&gt; vision set out by Herbert Simon in the latter half of the 20th century. Rather than focusing on departures from the expected utility model, Simon focused more on descriptive models and included notions of choice under uncertainty rather than exclusively choice under risk.&lt;/p&gt;

&lt;h2 id=&quot;intellectual-control&quot;&gt;Intellectual Control&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: George Fairbanks&lt;/p&gt;

&lt;p&gt;Publication: IEEE Software&lt;/p&gt;

&lt;p&gt;Published: Jan/Feb 2019&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1109/MS.2018.2874294&quot;&gt;10.1109/MS.2018.2874294&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article is about the notion of control in software development. The current practice of &lt;a href=&quot;https://en.wikipedia.org/wiki/Test-driven_development&quot;&gt;test driven development&lt;/a&gt; provides statistical control, i.e., if all of the test cases pass, then there likely is not a problem. In contrast, an alternative notion is intellectual control, i.e., being able to explain what the code/program/system is doing. There are obvious tradeoffs: intellectual control doesn’t scale well; statistical control can be effective if executed well; etc.&lt;/p&gt;

&lt;p&gt;The author provides an effective analogy that differentiates the two notions of control:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“imagine driving a car on a road with guardrails. In this metaphor, the guardrails are tests and driving the car is us writing programs. He then wondered if it’s OK to successfully arrive at our destination after hitting the guardrails during the journey. His audience laughed because having a car under control means we are able to drive without hitting the rails.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tony Hoare said it best in &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=358549.358561&quot;&gt;his Turing Award lecture&lt;/a&gt;, “There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies.”&lt;/p&gt;

&lt;h2 id=&quot;unicorns-cheshire-cats-and-the-new-dilemmas-of-entrepreneurial-finance&quot;&gt;Unicorns, Cheshire cats, and the new dilemmas of entrepreneurial finance&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Martin Kenney and John Zysman&lt;/p&gt;

&lt;p&gt;Publication: Venture Capital&lt;/p&gt;

&lt;p&gt;Published: Mar 2019&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1080/13691066.2018.1517430&quot;&gt;10.1080/13691066.2018.1517430&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper examines the rise of the unicorn: a start-up with a private valuation of over one billion USD. Many of these unicorns came about due to the low barrier to entry of starting a firm and raising capital. Often these firms are technology-focused with lots of intangible assets, e.g., Uber, AirBnB, etc., and are, thus, easily scalable. To scale up and gain market share, these companies often burn through massive amounts of capital while in a growth-phase. An ecosystem of funding, including traditional VC, angel investors, incubators, etc., has developed to progress these firms through their growth-phase with the eventual goal of selling their stake to another investor or to the public.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;60%&quot; height=&quot;60%&quot; src=&quot;/images/blog/top_articles_2019/cheshire.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A result is that incumbent firms in traditionally stable industries have been “disrupted” by growth-phase start-ups aiming to take market share. The attempt to capture market share is often done by offering lower prices, e.g., an Uber or Lyft ride is usually cheaper than getting a taxi. At the same time, the start-up is using capital money to fund this behavior. The fight for market share is motivated by the winner-takes-all nature of many industries, since there is not necessarily an market need for several search engines, social media platforms, etc.&lt;/p&gt;

&lt;p&gt;From an economic perspective, it is not clear if this method of producing firms is beneficial overall, since the goal for investors appears to be to raise the private valuation to sell to another investor or the public without concern for the long-term stability of the start-up.&lt;/p&gt;

&lt;p&gt;The authors provide a nice summary for their paper’s goal: “to interrogate the enthusiasm for backing entrepreneurial start-ups, losses or not, and for seeking to turbo-charge their growth to the point that they become the so-called ‘unicorns.’”&lt;/p&gt;

&lt;p&gt;This paper shared much in common with &lt;a href=&quot;https://press.princeton.edu/books/hardcover/9780691175034/capitalism-without-capital&quot;&gt;Capitalism without Capital&lt;/a&gt; which made the &lt;a href=&quot;## TODO&quot;&gt;2019 Top Books list&lt;/a&gt;. I came across this paper from &lt;a href=&quot;https://www.economist.com/briefing/2019/04/17/the-wave-of-unicorn-ipos-reveals-silicon-valleys-groupthink&quot;&gt;an article in The Economist&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-new-americanism&quot;&gt;A New Americanism&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Jill Lepore&lt;/p&gt;

&lt;p&gt;Publication: Foreign Affairs&lt;/p&gt;

&lt;p&gt;Published: Mar/Apr 2019&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.foreignaffairs.com/articles/united-states/2019-02-05/new-americanism-nationalism-jill-lepore&quot;&gt;www.foreignaffairs.com/articles/united-states/2019-02-05/new-americanism-nationalism-jill-lepore&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This short article argues that nations look to an origin story to explain their existence and that this story changes with the times. It wasn’t until the 1840s that the people of the United States viewed themselves as a nation (not sure what population this applies to). These shared myths continued to be developed until the second half of the twentieth century where they fell out of fashion for historians as nationalism yielded way to globalization, first politically then economically.&lt;/p&gt;

&lt;p&gt;Just because academic trends changed does not entail that the nation no longer looks for its founding myth; rather, these myths are just not provided by historians. The author alludes to this being the origin of the nativist, so-called nationalist movement observed in the US as well as in other countries. The author urges historians to write about the nation once more.&lt;/p&gt;

&lt;p&gt;Lepore’s history of the United States &lt;a href=&quot;https://www.thesetruthsbook.com&quot;&gt;These Truths&lt;/a&gt; was released in late 2018.&lt;/p&gt;

&lt;h2 id=&quot;the-dangers-of-post-hoc-interpretability-unjustified-counterfactual-explanations&quot;&gt;The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki&lt;/p&gt;

&lt;p&gt;Publication: Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)&lt;/p&gt;

&lt;p&gt;Published: Jul 2019&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.ijcai.org/proceedings/2019/0388.pdf&quot;&gt;ijcai.org/proceedings/2019/0388.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper extends the idea of a local explanation of a classifier’s prediction to counterfactually-justified local explanations to provide a link between the classifier and the training data. For an explanation to be justified for a given point in the feature space $x$, there must exist point $z$ in the training data, perhaps the nearest point to $x$, that’s correctly classified where $x, z$ belong to the same label and are topologically &lt;a href=&quot;https://en.wikipedia.org/wiki/Connected_space#Path_connectedness&quot;&gt;path connected&lt;/a&gt; with respect to the euclidean topology on the feature space. The intuition is that a local explanation for a prediction cannot be justified in this stronger sense if the point in question is isolated from correctly predicted instances in the training data. This leads to the amusing corollary that no explanations are justified if the predictive model classifies all instances in the training data incorrectly.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/blog/top_articles_2019/counterfactual_explanations.png&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center&quot;&gt;Left: explanation not justified; right: explanation justified&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;The authors operationalize this through an approximation algorithm to provide an estimate of whether or not a point is path connected to a correct prediction of an observation in the training data sharing the same label. A &lt;a href=&quot;https://www.ecmlpkdd2019.org/downloads/paper/226.pdf&quot;&gt;followup paper&lt;/a&gt; by this group appeared at KDD in August.&lt;/p&gt;

&lt;p&gt;I find this paper particularly compelling since it connects closely with my work studying the &lt;a href=&quot;https://arxiv.org/abs/1910.08595v2&quot;&gt;topological properties of explanations of classifier predictions&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;mistaken-i-think-therefore-i-make-mistakes-and-change-my-mind&quot;&gt;Mistaken: I think, therefore I make mistakes and change my mind&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Daniel Ward&lt;/p&gt;

&lt;p&gt;Publication: Aeon&lt;/p&gt;

&lt;p&gt;Published: Oct 2019&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://aeon.co/essays/i-think-therefore-i-make-mistakes-and-change-my-mind&quot;&gt;aeon.co/essays/i-think-therefore-i-make-mistakes-and-change-my-mind&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article is about the notion of human infallibility and how it is antithetical to rationality. On this conception, being rational is being able to give reasons for one’s actions and that involves the possibility of error. The author argues that the notion of human infallibility underlies both &lt;a href=&quot;https://plato.stanford.edu/entries/moral-relativism/&quot;&gt;moral relativism&lt;/a&gt; in anthropology/sociology and &lt;a href=&quot;https://en.wikipedia.org/wiki/Revealed_preference&quot;&gt;revealed preference&lt;/a&gt; in economics, both of which the author finds objectionable.&lt;/p&gt;

&lt;p&gt;The case of revealed preference is the more interesting of the two. Revealed preference is the notion that an agent reveals their preferences for goods through their choices, a behaviorist approach to choice/decision theory. The author argues that this theory does not account for genuine mistakes. We may think that is because the agent in the model is more rational than humans; however, this turns on the specific notion of rationality. On the reason giving account, mistakes are the hallmark of rationality.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“To err is human. Missteps, misapprehensions, misspeakings, momentary lapses and mess-ups are part of the fabric of life. Yet we are capable of making mistakes precisely because we are thoughtful, intelligent beings with complex goals and sincerely held values. We wouldn’t be able to if we were otherwise. Regrets: we’ve had a few. But we are the wiser for them.”&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">Below are the top eleven articles I’ve read in 2019. A theme of methodology runs through this set of papers, especially statistical methodology. There’s also some fun miscellany mixed in with blockchain (whose craze seems like a lifetime ago now), unicorns, and the history of the English language. To my surprise, all of these articles are from the present decade. They are presented in chronological order.</summary></entry><entry><title type="html">Resources for Learning Measure Theory</title><link href="http://localhost:4000/measure_theory_resources/" rel="alternate" type="text/html" title="Resources for Learning Measure Theory" /><published>2019-10-13T00:00:00-05:00</published><updated>2019-10-13T00:00:00-05:00</updated><id>http://localhost:4000/measure_theory_resources</id><content type="html" xml:base="http://localhost:4000/measure_theory_resources/">&lt;p&gt;When approaching measure theory for the first time, the ideas can seem opaque and unmotivated. This is amplified since many students of measure theory are not coming from a strictly mathematics background and may be approaching the material on their own outside of the classroom. In addition to first-year math graduate students and advanced math undergraduates, students in stats, economics, the hard sciences, etc. will find their way into learning measure theory. This is a guide to resources for learning measure theory that tries to keep in mind that many (myself included) approach the material with an atypical background.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;To work through the material for measure theory, one should be familiar with the concepts and basic results from real analysis (limit, sup/inf, metric, etc.) and have some exposure to general topology (open sets, closed sets, continuity, etc.). While not strictly necessary depending on one’s background, it’s helpful to have exposure to both the differential calculus and probability to have mental models available to check against these measure-theoretic ideas. Francis Su provides an excellent &lt;a href=&quot;https://analysisyawp.blogspot.com/2013/01/lectures.html&quot;&gt;set of video lectures&lt;/a&gt; to get one up to speed on real analysis. As for the elements of topology, I suggest taking a look at the book &lt;a href=&quot;http://www.topologywithouttears.net&quot;&gt;&lt;em&gt;Topology without Tears&lt;/em&gt;&lt;/a&gt; by Sidney A. Morris, available for free in pdf form.&lt;/p&gt;

&lt;h2 id=&quot;books&quot;&gt;Books&lt;/h2&gt;

&lt;p&gt;I recommend two texts: Richard Bass’ &lt;em&gt;Real Analysis for Graduate Students&lt;/em&gt; as the primary text and David Bressoud’s &lt;em&gt;A Radical Approach to Lebesgue’s Theory of Integration&lt;/em&gt; as a supplement.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;http://bass.math.uconn.edu/real-cover.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there are stacks upon stacks of measure theory texts available, the Bass book combines instructive proofs with an organization that motivates the material and a collection of interesting and (sometimes) difficult exercises. These more difficult exercises will be particularly useful for those studying for Comps/Quals. The selling point is that all versions of the book are available in pdf form on the &lt;a href=&quot;http://bass.math.uconn.edu/real.html&quot;&gt;author’s website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A typical course in measure theory will take one through chapter fifteen. This starts with the definition of a measure on sets (1-4) to a measure on a function (5) to integration and differentiation of functions (6-14) and, finally, to $\mathcal{L}_p$ spaces of functions (15). The Bass book includes chapters on topology (20) and measure-theoretic probability (21) for foundations and applications, respectively; however, these sections are not as well put-together as the first half of the book. With that being said, this book contains everything one needs to get a handle on measure theory in a reasonably digestible form.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: left; display: inline-block; margin: 0px 20px 0px 0px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://www.macalester.edu/~bressoud/books/ARattleToy.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the Bass book contains the meat of a course on measure theory, it often lacks the context one may find in a classroom discussion. Bressoud’s &lt;a href=&quot;https://www.cambridge.org/us/academic/subjects/mathematics/abstract-analysis/radical-approach-lebesgues-theory-integration?format=PB&amp;amp;isbn=9780521711838&quot;&gt;&lt;em&gt;A Radical Approach to Lebesgue’s Theory of Integration&lt;/em&gt;&lt;/a&gt; approaches the Lebesgue integral and measure theory more generally from a historical perspective. This book need not be read cover-to-cover; I’ve found it is more useful to skip around to relevant sections. I &lt;em&gt;do&lt;/em&gt; recommend reading chapters three, four, and five on the current conception of $\mathbb{R}$, the problem posed to Riemann integration by nowhere dense sets, and the development of early measure theory.&lt;/p&gt;

&lt;h2 id=&quot;video-lectures&quot;&gt;Video Lectures&lt;/h2&gt;

&lt;p&gt;It can often be difficult to learn streamlined material developed for a first-year graduate course outside the context of that course. For some people (myself included), hearing and seeing someone walkthrough an example can be the difference between internalizing the example to build intuition and second-guessing the result each time a similar case comes up.&lt;/p&gt;

&lt;p&gt;If this sounds like you, the &lt;a href=&quot;https://www.youtube.com/playlist?list=PLo4jXE-LdDTQq8ZyA8F8reSQHej3F6RFX&quot;&gt;collection of measure theory lectures&lt;/a&gt; from &lt;a href=&quot;http://w3.impa.br/~landim/&quot;&gt;Claudio Landim&lt;/a&gt; will be a great help! Landim’s lectures provide a comprehensive measure theory course that captures the feel of a classroom with over 30 hours of material. Landim is particularly good with providing insightful examples and helping the viewer focus on the key steps in proofs.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?!&lt;/h2&gt;

&lt;p&gt;Getting a handle on the basics of measure theory allows one pursue numerous areas of mathematics and its applications. Below are some that I find interesting and recommendations for a first dive into each area.&lt;/p&gt;

&lt;h3 id=&quot;measure-theoretic-probability&quot;&gt;Measure-theoretic Probability&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;http://probability.ca/jeff/grprobcov2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Measure theory is the most common foundation for a rigorous treatment of probability. Many of the odd rules one sees in an initial treatment of probability are reduced to questions of measure theory. For instance, rather than having a rule or heuristic saying that the probability of a continuous random variable taking the value of a single point is zero, we can observe that a probability measure with a continuous density function is absolutely continuous to the Lebesgue measure and that a single point is a Lebesgue null or measure zero set.&lt;/p&gt;

&lt;p&gt;For this, I recommend Jeffrey Rosenthal’s &lt;a href=&quot;http://probability.ca/jeff/grprobbook.html&quot;&gt;A First Look at Rigorous Probability Theory&lt;/a&gt;. This book also opens up paths to explore in financial mathematics, stochastic process models, etc.&lt;/p&gt;

&lt;h3 id=&quot;topology&quot;&gt;Topology&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://www.pearsonhighered.com/assets/bigcovers/0/1/3/1/0131816292.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In typical treatments of measure theory, one begins by working with the Lebesgue measure in euclidean space and then generalizes to measures on other sorts of spaces, e.g., the counting measure on the space formed by subsets of the natural numbers. Though more abstract, these spaces are but one sort of topological space. Point-set topology is the study of properties of and between general topological spaces.&lt;/p&gt;

&lt;p&gt;The recommended text in this area is the first half of James Munkres’ &lt;a href=&quot;https://www.pearson.com/us/higher-education/program/Munkres-Topology-2nd-Edition/PGM56881.html&quot;&gt;&lt;em&gt;Topology&lt;/em&gt;&lt;/a&gt;. The second half of this book devoted to Algebraic Topology, a different perspective on studying topological spaces; however, better treatments exist of this latter topic. While Munkres can be terse at times, his dark humor is undeniable: see the exposition beginning Section 33 on the Urysohn Lemma. I read it as humor anyhow.&lt;/p&gt;

&lt;h3 id=&quot;measure-theory-meets-topology&quot;&gt;Measure Theory meets Topology&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://images.springer.com/sgw/books/medium/9780387905082.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In measure theory, we have a notion of small or negligible sets: null sets. A corresponding notion of small or negligible sets in a topological space is that of being &lt;a href=&quot;https://en.wikipedia.org/wiki/Meagre_set&quot;&gt;meagre&lt;/a&gt;, i.e., the countable union of nowhere dense sets. John Oxtoby’s &lt;a href=&quot;https://www.springer.com/gp/book/9780387905082&quot;&gt;&lt;em&gt;Measure and Category&lt;/em&gt;&lt;/a&gt; studies analogies between these two concepts and proves “duality results” on when we can interchange meagre for null sets (and vice versa) in theorems.&lt;/p&gt;

&lt;p&gt;One aspect of meagre sets being negligible is that they contain no non-trivial open subsets. One may be tempted to think that the meagre sets are just the null sets on the reals; however, &lt;a href=&quot;https://mathoverflow.net/questions/43478/is-there-a-measure-zero-set-which-isnt-meagre&quot;&gt;this is not the case&lt;/a&gt;. We can partition $\mathbb{R}$ into a null set and a meagre set of low Borel rank.&lt;/p&gt;

&lt;h3 id=&quot;descriptive-set-theory&quot;&gt;Descriptive Set Theory&lt;/h3&gt;

&lt;p&gt;Descriptive set theory studies the structure and properties of “well-behaved” subsets of the reals and similar but more general spaces called &lt;a href=&quot;https://en.wikipedia.org/wiki/Polish_space&quot;&gt;polish spaces&lt;/a&gt;. One starting place is the structure of the Borel sets, i.e., the closure of the open sets by repeated application of countable union, countable intersection, and complementation. An introduction in this direction is S. M. Srivastava’s &lt;a href=&quot;https://www.springer.com/gp/book/9780387984124&quot;&gt;&lt;em&gt;A Course on Borel Sets&lt;/em&gt;&lt;/a&gt;. I will also point to a set of accessible &lt;a href=&quot;http://www.personal.psu.edu/jsr25/Spring_11/574_Sp11_Syllabus.html&quot;&gt;lecture notes&lt;/a&gt; by Jan Reimann.&lt;/p&gt;

&lt;p&gt;This area forms the foundation for one of my primary interests: topological learning theory (link coming soon).&lt;/p&gt;

&lt;h3 id=&quot;history-of-measure-theory&quot;&gt;History of Measure Theory&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://www.maa.org/sites/default/files/HawkinsLebesque.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you’ve read Bressoud’s text that I offer as a supplement above, then you’ll be familiar with Thomas Hawkins’ &lt;a href=&quot;https://bookstore.ams.org/chel-282/&quot;&gt;&lt;em&gt;Lebesgue’s Theory of Integration: Its Origins and Developments&lt;/em&gt;&lt;/a&gt;. Hawkins’ book seems to be &lt;em&gt;the&lt;/em&gt; comprehensive account of the early history of the Lebesgue integral and the abstract measure theory that developed from it. I’ve not yet made it all the way through this one; it is on my list through!&lt;/p&gt;</content><author><name></name></author><summary type="html">When approaching measure theory for the first time, the ideas can seem opaque and unmotivated. This is amplified since many students of measure theory are not coming from a strictly mathematics background and may be approaching the material on their own outside of the classroom. In addition to first-year math graduate students and advanced math undergraduates, students in stats, economics, the hard sciences, etc. will find their way into learning measure theory. This is a guide to resources for learning measure theory that tries to keep in mind that many (myself included) approach the material with an atypical background.</summary></entry><entry><title type="html">Parsing Nested JSON Records in Python</title><link href="http://localhost:4000/parsing-json-python/" rel="alternate" type="text/html" title="Parsing Nested JSON Records in Python" /><published>2019-06-22T00:00:00-05:00</published><updated>2019-06-22T00:00:00-05:00</updated><id>http://localhost:4000/parsing-json-python</id><content type="html" xml:base="http://localhost:4000/parsing-json-python/">&lt;p&gt;JSON is the typical format used by web services for message passing that’s also relatively human-readable. Despite being more human-readable than most alternatives, JSON objects can be quite complex. For analyzing complex JSON data in Python, there aren’t clear, general methods for extracting information (see &lt;a href=&quot;https://realpython.com/python-json/&quot;&gt;here&lt;/a&gt; for a tutorial of working with JSON data in Python). This post provides a solution if one knows the path through the nested JSON to the desired information.&lt;/p&gt;

&lt;h2 id=&quot;motivating-example&quot;&gt;Motivating Example&lt;/h2&gt;
&lt;p&gt;Suppose you have the following JSON record:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;employees&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Alice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Bob&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;firm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Charlie's Waffle Emporium&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;location&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CA&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This record has two keys at the top level: &lt;em&gt;employees&lt;/em&gt; and &lt;em&gt;firm&lt;/em&gt;. The value for the &lt;em&gt;employees&lt;/em&gt; key is a list of two objects of the same schema; each object has the keys &lt;em&gt;name&lt;/em&gt;, &lt;em&gt;role&lt;/em&gt;, and &lt;em&gt;nbr&lt;/em&gt;. The value for the &lt;em&gt;firm&lt;/em&gt; key is an object with the keys &lt;em&gt;name&lt;/em&gt; and &lt;em&gt;location&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Suppose you want to extract the names of the employees. This record will give problems for approaches that just search through key names, since the name of the firm will be returned as well.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;Calling the &lt;em&gt;extract_element_from_json&lt;/em&gt; function on the above record delivers the desired result:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;employees&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Alice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Bob&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;firm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Charlie's Waffle Emporium&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;location&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;CA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;extract_element_from_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;employees&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Alice'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Bob'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;
&lt;p&gt;This function nests into the record(s) in &lt;em&gt;obj&lt;/em&gt; according to the keys specified in &lt;em&gt;path&lt;/em&gt; to retrieve the desired information. When a list is encountered as the value of a key in &lt;em&gt;path&lt;/em&gt;, this function splits and continues nesting on each element of the encountered list in a depth-first manner. This is how both ‘Alice’ and ‘Bob’ are returned; since the value of &lt;em&gt;employees&lt;/em&gt; is a list, the nesting is split on both of its elements and each of the values for &lt;em&gt;name&lt;/em&gt; are appended to the output list.&lt;/p&gt;

&lt;p&gt;If &lt;em&gt;obj&lt;/em&gt; is a single dictionary/JSON record, then this function returns a list containing the desired information, and if &lt;em&gt;obj&lt;/em&gt; is a list of dictionaries/JSON records, then this function returns a list of lists containing the desired information.&lt;/p&gt;

&lt;p&gt;If any element of &lt;em&gt;path&lt;/em&gt; is missing from the corresponding level of the nested dictionary/JSON, then this function returns a &lt;em&gt;None&lt;/em&gt; .&lt;/p&gt;

&lt;p&gt;Below is the full function (inspired/motivated from what’s discussed &lt;a href=&quot;https://hackersandslackers.com/extract-data-from-complex-json-python/&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;extract_element_from_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''
    Extracts an element from a nested dictionary or
    a list of nested dictionaries along a specified path.
    If the input is a dictionary, a list is returned.
    If the input is a list of dictionary, a list of lists is returned.
    obj - list or dict - input dictionary or list of dictionaries
    path - list - list of strings that form the path to the desired element
    '''&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'''
            Extracts an element from a nested dictionary
            along a specified path and returns a list.
            obj - dict - input dictionary
            path - list - list of strings that form the JSON path
            ind - int - starting index
            arr - list - output list
        '''&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outer_arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outer_arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outer_arr&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;update&quot;&gt;Update&lt;/h2&gt;
&lt;p&gt;This post is featured in Issue #374 of &lt;a href=&quot;https://pycoders.com/issues/374&quot;&gt;PyCoder’s Weekly&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">JSON is the typical format used by web services for message passing that’s also relatively human-readable. Despite being more human-readable than most alternatives, JSON objects can be quite complex. For analyzing complex JSON data in Python, there aren’t clear, general methods for extracting information (see here for a tutorial of working with JSON data in Python). This post provides a solution if one knows the path through the nested JSON to the desired information.</summary></entry></feed>