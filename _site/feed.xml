<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-22T03:20:31-06:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Brett Mullins</title><subtitle>Researcher - Data Scientist</subtitle><entry><title type="html">Top Articles I’ve Read in 2018</title><link href="http://localhost:4000/Top-Articles-2018/" rel="alternate" type="text/html" title="Top Articles I've Read in 2018" /><published>2018-12-16T00:00:00-06:00</published><updated>2018-12-16T00:00:00-06:00</updated><id>http://localhost:4000/Top-Articles-2018</id><content type="html" xml:base="http://localhost:4000/Top-Articles-2018/">&lt;p&gt;Here are the top eleven papers I’ve come across in 2018.$^*$ These papers are mostly recent publications (within the last two years) with some older ones peppered in. They are in chronological order below.&lt;/p&gt;

&lt;p&gt;For the top books that I’ve read in 2018, &lt;a href=&quot;/Top-Books-2018/&quot;&gt;click here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pick-the-largest-number&quot;&gt;Pick the Largest Number&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Thomas Cover&lt;/p&gt;

&lt;p&gt;Publication: Open Problems in Communication and Computation&lt;/p&gt;

&lt;p&gt;Published: 1987&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1007/978-1-4612-4808-8&quot;&gt;10.1007/978-1-4612-4808-8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This short note describes a solution to the two-card variant of a game called Googol. In this variant of the game, Player 1 writes down two distinct rational numbers $x_1, x_2$ and Player 2 guesses which of the two is larger after seeing the first number $x_1$. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_indifference&quot;&gt;principle of indifference&lt;/a&gt; gives a probability of success of one half by random guessing. The paper offers a strictly better solution. Pick a threshold $T$ from density $f(t)$ where $f(t)$ has support on the reals, e.g., the standard normal distribution. If $x_1 &amp;lt; T$, then say $x_2$ is larger and vice versa. Since there is a non-zero probability that $T$ is between $x_1$ and $x_2$, this strategy strictly dominates random guessing.&lt;/p&gt;

&lt;p&gt;On a first reading, this is an unexpected result that took a bit to wrap my mind around. Note that this conclusion does not violate the principle of indifference since Player 2 is not in a state of ignorance; they have observed the value of $x_1$. What makes this problem interesting is that $x_2$ does not depend on the value of $x_1$ so knowing $x_1$ seems irrelevant to determining the size of $x_2$.&lt;/p&gt;

&lt;p&gt;I came across this paper by way of the &lt;a href=&quot;https://fermatslibrary.com/journal_club&quot;&gt;Fermat’s Library Journal Club&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-trouble-with-psychological-darwinism&quot;&gt;The Trouble with Psychological Darwinism&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://static01.nyt.com/images/2017/12/01/arts/01fodor-obit-1/01fodor-obit-1-superJumbo.jpg?quality=90&amp;amp;auto=webp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Author: Jerry Fodor&lt;/p&gt;

&lt;p&gt;Publication: London Review of Books&lt;/p&gt;

&lt;p&gt;Published: Jan 1998&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.lrb.co.uk/v20/n02/jerry-fodor/the-trouble-with-psychological-darwinism&quot;&gt;www.lrb.co.uk/v20/n02/jerry-fodor/the-trouble-with-psychological-darwinism&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jerry Fodor reviews “How the Mind Works” by Stephen Pinker and “Evolution in Mind” by Henry Plotkin. These thinkers fall in line with the New Rationalism view of the mind which holds that the mind functions from modular structures and is partly innate (or the structures are anyway). This is in contrast to the blank-slate, social constructionist, plastic empiricist view. Fodor lays out four principles of Pinker’s and Plotkin’s views and points out where they deviate from his own. Here are the four: 1. the mind is a computational system; 2. the mind is largely modular; 3. a lot of cognitive structure is innate; 4. a lot of mental structure is an evolutionary adaption. Fodor contests the fourth point by adding that 3. doesn’t imply 4.&lt;/p&gt;

&lt;p&gt;A wonderful read that I couldn’t endorse more. Fodor’s writing is both lucid and highly entertaining. Here is a passage characteristic of this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Psychological Darwinism is a kind of conspiracy theory; that is, it explains behaviour by imputing an interest (viz in the proliferation of the genome) that the agent of the behaviour does not acknowledge. When literal conspiracies are alleged, duplicity is generally part of the charge: ‘He wasn’t making confetti; he was shredding the evidence. He did X in aid of Y, and then he lied about his motive.’ But in the kind of conspiracy theories psychologists like best, the motive is supposed to be inaccessible even to the agent, who is thus perfectly sincere in denying the imputation. In the extreme case, it’s hardly even the agent to whom the motive is attributed. Freudian explanations provide a familiar example: What seemed to be merely Jones’s slip of the tongue was the unconscious expression of a libidinous impulse. But not Jones’s libidinous impulse, really; one that his Id had on his behalf. Likewise, for the psychological Darwinist: what seemed to be your, after all, unsurprising interest in your child’s well-being turns out to be your genes’ conspiracy to propagate themselves. Not your conspiracy, notice, but theirs.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nytimes.com/2017/11/30/obituaries/jerry-a-fodor-dead-philosopher-of-the-mind.html&quot;&gt;Jerry Fodor passed away in late 2017.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-mathematicians-lament&quot;&gt;A Mathematician’s Lament&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Paul Lockhart&lt;/p&gt;

&lt;p&gt;Published: 2002&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;http://mysite.science.uottawa.ca/mnewman/LockhartsLament.pdf&quot;&gt;http://mysite.science.uottawa.ca/mnewman/LockhartsLament.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This essay is an indictment of the modern K-12 math curriculum primarily because mathematics is presented as unmotivated, algorithmic drudgery by educators who have never done mathematics. This is a wonderful and hilarious read! The author argues that mathematics classes should be less structured, more argument based, and concepts and distinctions should be situated in their historical and philosophical context.&lt;/p&gt;

&lt;p&gt;The author runs with the analogy of painting where we would never teach painting without having the students actually paint. The proper analogue with the current mathematics curriculum would be having the students memorize colors, brushes, styles, and terse terminology with no motivation or context and not ever really have them paint except in some rigidly defined sense, e.g., painting by numbers or tracing from a stencil.&lt;/p&gt;

&lt;p&gt;The view is pushed that mathematics is an art whose practical consequences are a biproduct rather than an end. This is similar to those who argue that philosophy is an activity rather than a subject.&lt;/p&gt;

&lt;h2 id=&quot;to-explain-or-to-predict&quot;&gt;To Explain or to Predict&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Galit Shmueli&lt;/p&gt;

&lt;p&gt;Publication: Statistical Science&lt;/p&gt;

&lt;p&gt;Published: Aug 2010&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1214/10-STS330&quot;&gt;10.1214/10-STS330&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper covers the difference between prediction and explanation in depth. It discusses the difference in the modeling process from conception to model validation featuring two case studies with the Netflix Challenge data and auction data. The author distinguishes descriptive modeling from the tasks of prediction and explanation; however, descriptive modeling is not covered in depth.&lt;/p&gt;

&lt;p&gt;While this paper is not groundbreaking, it provides an excellent overview between different sorts of modeling especially for those coming from machine learning-heavy or statistics-heavy backgrounds. The appendix provides an example of when a properly specified model, i.e., the true model, may not be the most predictive. Though counterintuitive at first, this result demonstrates the fundamental difference between the tasks of prediction and explanation.&lt;/p&gt;

&lt;h2 id=&quot;progress-in-evidence-based-medicine-a-quarter-century-on&quot;&gt;Progress in evidence-based medicine: a quarter century on&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://s3.amazonaws.com/libapps/customers/4658/images/EBM.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Authors: Benjamin Djulbegovic &amp;amp; Gordon H Guyatt&lt;/p&gt;

&lt;p&gt;Publication: The Lancet&lt;/p&gt;

&lt;p&gt;Published: Feb 2017&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1016/S0140-6736(16)31592-6&quot;&gt;doi.org/10.1016/S0140-6736(16)31592-6&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article introduces Evidence Based Medicine (EBM) and it’s three main tenets: 1. not all evidence is created equal; 2. all relevant evidence should be used in decision making; 3. clinical decision making should consider patients’ values and preferences. Initially, the field pushed for practitioners to remain up-to-date on the latest research in their field; however, this is too tall of a task, so the movement changed emphasis to systematic studies and literature overviews.&lt;/p&gt;

&lt;p&gt;One prominent criticism addressed is that this leads to algorithmic decision making that is divorced from the human element. This is true is two respects: 1. the reasoning may be too complex for the patient to understand; 2. the individual practitioner’s influence is greatly reduced.&lt;/p&gt;

&lt;p&gt;I found this paper particularly interesting given that there’s an close analogy between EBM and data science that would have not been apparent prior to working in industry.&lt;/p&gt;

&lt;p&gt;This paper was featured in the EBM column of an issue of &lt;a href=&quot;http://www.thereasoner.org&quot;&gt;The Reasoner&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;understanding-deep-learning-requires-rethinking-generalization&quot;&gt;Understanding Deep Learning Requires Rethinking Generalization&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, &amp;amp; Oriol Vinyals&lt;/p&gt;

&lt;p&gt;Publication: arXiv&lt;/p&gt;

&lt;p&gt;Published: Feb 2017&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;arxiv.org/abs/1611.03530&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this paper, the authors present empirical and theoretical results on generalization error and the capacity for memorization in deep learning models, specifically with convolutional neural networks (CNN). CNN architectures exist that achieve near-perfect performance on standard image datasets, e.g. CIFAR10, with low generalization error. The authors found that by randomly permuting the image labels, the same architectures produce models with near-perfect accuracy on the training data but, correspondingly, a high generalization error. They summarize their finding as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Deep neural networks easily fit random labels.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To put this result into context, the authors explore how different regularization techniques, e.g., dropout, weight decay, etc., affect generalization error. The conclusion is along the lines of &lt;em&gt;we don’t know what causes neural networks to generalize well.&lt;/em&gt; I do want to emphasize that while I found this paper interesting, readable, and helpful to understanding deep learning models, it may have received more hype from the ML community than it deserves.&lt;/p&gt;

&lt;p&gt;I crossed paths with this paper while browsing &lt;a href=&quot;https://blog.acolyer.org&quot;&gt;the morning paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stuck-the-law-and-economics-of-residential-stability&quot;&gt;Stuck! The Law and Economics of Residential Stability&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: David Schleicher&lt;/p&gt;

&lt;p&gt;Publication: Yale Law Journal&lt;/p&gt;

&lt;p&gt;Published: October 2017&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.yalelawjournal.org/article/stuck-the-law-and-economics-of-residential-stagnation&quot;&gt;https://www.yalelawjournal.org/article/stuck-the-law-and-economics-of-residential-stagnation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article discusses the phenomena of declining interstate mobility and some of its potential causes and effects. To summarize the decline:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Americans are not leaving places hit by economic crises, resulting in unemployment rates and low wages that linger in these areas for decades. And people are not moving to rich regions where the highest wages are available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The author examines the state and local policies that affect interstate mobility both those that impede entering a new state, e.g., differential licensure requirements, and those that impede leaving one’s current state, e.g., encouraging homeownership through the mortgage interest deduction and other subsidies. One effect discussed extensively of the sluggish labor market resulting from decreased interstate mobility is the decreased effectiveness of federal macroeconomic policy. Consider two neighboring states where one state is booming while the other busts but there is little population movement between the states. In this case, there may not be a uniform monetary policy beneficial to both states.&lt;/p&gt;

&lt;p&gt;This article is interesting primarily as a dip of one’s toes into the depths of economic mobility.&lt;/p&gt;

&lt;p&gt;I came across this article in the Further Reading from the &lt;a href=&quot;https://www.aeaweb.org/journals/jep&quot;&gt;Journal of Economic Perspectives&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;top5itis&quot;&gt;TOP5ITIS&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Roberto Serrano&lt;/p&gt;

&lt;p&gt;Status: Working paper as of Jan 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;http://www.econ.brown.edu/Faculty/serrano/pdfs/wp2018-2-Top5itis.pdf&quot;&gt;www.econ.brown.edu/Faculty/serrano/pdfs/wp2018-2-Top5itis.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper is a commical commentary on both the academic economists’ infatuation with the &lt;em&gt;Top5&lt;/em&gt; economics journals and the recent discussions of them at the AEA meetings. The author charges the practice of focusing primairly on &lt;em&gt;Top5&lt;/em&gt; publications as arbitrary gatekeeping that potentially stifles innovation that is not clearly a good measure of either one’s performance as an academic or one’s expertise in a topic. A sample of the goodness:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Since the disease is a simple application of the counting measure (typically a person learns to count in primary school), through a process of contagion, top5itis spreads quickly to affect people outside economics, including schoolchildren offspring of economists who get together in the playground to make disparaging remarks about each other’s parents.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;learning-is-a-risky-business&quot;&gt;Learning is a Risky Business&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Wayne C. Myrvold&lt;/p&gt;

&lt;p&gt;Publication: Erkenntnis&lt;/p&gt;

&lt;p&gt;Published: Feb 2018&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1007/s10670-018-9972-0&quot;&gt;10.1007/s10670-018-9972-0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper advances an argument against the &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_indifference&quot;&gt;principle of indifference&lt;/a&gt;. Roughly, the principle of indifference says that absent evidence to the contrary one should assign equal probability to each possible state of the world. The author’s argument proceeds by introducing time into the partitioning of the state space. Simply put: consider a year where each day either $\phi$ or $\neg \phi$. Then there are $2^{365}$ possible histories. The principle of indifference assigns equal probability to each; however, this implies that the assessment of probabilities on each day is independent of what happened on prior days, i.e., one does not learn from experience.&lt;/p&gt;

&lt;p&gt;It’s a compelling argument that may require the notion of &lt;em&gt;learn&lt;/em&gt; to be tightened up. Also: the paper’s leading example involves a SuperBaby.&lt;/p&gt;

&lt;h2 id=&quot;anchors-high-precision-model-agnostic-explanations&quot;&gt;Anchors: High-Precision Model-Agnostic Explanations&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;40%&quot; height=&quot;40%&quot; src=&quot;/images/blog/best_articles_2018/anchor.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Authors: Marco Túlio Ribeiro, Sameer Singh, &amp;amp; Carlos Guestrin&lt;/p&gt;

&lt;p&gt;Publication: Proceedings of The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)&lt;/p&gt;

&lt;p&gt;Published: Feb 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982&quot;&gt;https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper introduces an approach to prediction explanation called anchors as a refinement of the LIME approach. With LIME, model behavior is assessed by sampling points around the observation of interest and building an interpretable model on the sampled data to find out which variables are driving the prediction. More on LIME &lt;a href=&quot;https://homes.cs.washington.edu/~marcotcr/blog/lime/&quot;&gt;here&lt;/a&gt;. One difficulty with building local models for post-hoc prediction is that it is not clear how far out from the observation of interest that the local model generalizes. This concern is addressed by introducing anchors, topologically simple regions of the feature space where a prediction holds with high probability. Think of anchors as a sufficiency condition. This method can better generalize because it is essentially claiming less. Note that at present the anchor approach only works with binary classification models.&lt;/p&gt;

&lt;p&gt;The GitHub repo to test out these techniques can be &lt;a href=&quot;https://github.com/marcotcr/anchor&quot;&gt;found here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;understanding-and-misunderstanding-randomized-controlled-trials&quot;&gt;Understanding and misunderstanding randomized controlled trials&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Angus Deaton and Nancy Cartwright&lt;/p&gt;

&lt;p&gt;Publication: Social Science &amp;amp; Medicine&lt;/p&gt;

&lt;p&gt;Published: Aug 2018&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1016/j.socscimed.2017.12.005&quot;&gt;10.1016/j.socscimed.2017.12.005&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article evaluates the benefits and limitations of randomized control trials (RCT). In practice, the goal of running an RCT is to obtain an estimate of the average effect from some treatment employing as few assumptions as possible; indeed, estimating the average treatment effect (ATE) is a nonparametric method. We often think of an RCT is the “gold-standard” evidence for the existence of a causal effect in medicine, economics, etc.; however, the effectiveness of an RCT comes from having no prior evidence. If we have more information about the underlying causal mechanism, then we can perform more accurate inference by stratifying our randomization by causal pathways, for instance, or employ various other causal modeling techniques. The value of RCTs is in when we do not have the same assumptions; however, once we agree on assumptions, we can leverage those assumptions to use other estimation methods.&lt;/p&gt;

&lt;p&gt;Applying the results of an RCT is tricky. Generalizing from a sample to a population is suspect in the social sciences due to all of the potential differences between groups, e.g., the sample may not have been random or there may be been post-randomization confounding, also called realized confounding. Often the estimated ATE is applied to the individual; however, this is entirely erroneous, since the ATE is an average and an individual’s characteristics will differ from the average.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“In these conversations, the results of an RCT may have marginal value. If your physician tells you that she endorses evidence-based medicine, and that the drug will work for you because an RCT has shown that ‘it works’, it is time to find a physician who knows that you and the average are not the same.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://causality.cs.ucla.edu/blog/index.php/2018/03/10/challenging-the-hegemony-of-randomized-controlled-trials-comments-on-deaton-and-cartwright/&quot;&gt;See here&lt;/a&gt; for comments on this paper from Judea Pearl.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;$^*$Since we’re likely not Pythagoreans, there’s probably not a good reason for preferring ten articles to eleven.&lt;/p&gt;</content><author><name></name></author><summary type="html">Here are the top eleven papers I’ve come across in 2018.$^*$ These papers are mostly recent publications (within the last two years) with some older ones peppered in. They are in chronological order below.</summary></entry><entry><title type="html">Top Books I’ve Read in 2018</title><link href="http://localhost:4000/Top-Books-2018/" rel="alternate" type="text/html" title="Top Books I've Read in 2018" /><published>2018-12-16T00:00:00-06:00</published><updated>2018-12-16T00:00:00-06:00</updated><id>http://localhost:4000/Top-Books-2018</id><content type="html" xml:base="http://localhost:4000/Top-Books-2018/">&lt;p&gt;Here are the top three books I’ve read in 2018. They are presented below in chronological order. While these three books seem rather disparate, they are bound together by themes of innovation, conflict, and ideology.&lt;/p&gt;

&lt;p&gt;For the top articles that I’ve read in 2018, &lt;a href=&quot;/Top-Articles-2018/&quot;&gt;click here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;science-and-change-1500-to-1700&quot;&gt;Science and Change: 1500 to 1700&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Hugh Kearney&lt;/p&gt;

&lt;p&gt;Published: 1971&lt;/p&gt;

&lt;p&gt;This book argues that the story of science’s emergence from the darkness during the Renaissance is an instance of revisionist history. &lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;20%&quot; height=&quot;20%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/3/31/Boyle_air_pump.jpg&quot; /&gt; This story is usually centered on Bacon, the Puritan spirit, the rise of Capitalism, experimentalism, and the founding of the Royal Society in England. In contrast, the author distinguishes between three dominant movements of thought between 1500 to 1700. The Organic movement dominated the beginning of the period having the authority of both church and state. This movement’s intellectual origins are found in Aristotle and biological analogies. The Mystical movement was a reaction to the Organic movement rooted in neo-platonism and alchemy. Finally, we have the usual protagonist in this story: the mechanist movement.&lt;/p&gt;

&lt;p&gt;The usual account holds that the mechanistic approach won out because the mechanists were also the experimentalists. With regard to experiments, differing motivations between practitioners in each tradition led to differences in the objects of experiments. &lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;30%&quot; height=&quot;30%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/William_Harvey_%281578-1657%29_Venenbild.jpg/1024px-William_Harvey_%281578-1657%29_Venenbild.jpg&quot; /&gt; For instance, the Organic approach experimented with blood flow and the purpose of the heart. We can see this illustrated in inventions as well: the Mystic approach made progress in the construction of furnaces, while the mechanists created better clockwork technology. So practitioners in all traditions experimented and invented; it was simply their motivations that differed.&lt;/p&gt;

&lt;p&gt;This book treads familiar ground of philosophy of science and general history of science. I had known that both Newton and Bacon was not the mechanists of their legacies. With that being said, I was unaware of the influence of neo-platonism and the Magical/Occult tradition during the Renaissance. This is a topic I intend to dive deeper into given my interests in Orphism and the so-called mystery cults of antiquity (an interest stemming from discussions of Dionysus and Bacchus in my all time favorite book &lt;a href=&quot;https://en.wikipedia.org/wiki/A_History_of_Western_Philosophy&quot;&gt;&lt;em&gt;A History of Western Philosophy&lt;/em&gt;&lt;/a&gt; by Bertrand Russell).&lt;/p&gt;

&lt;p&gt;The World Scientific printing of this book contains dozens of beautiful, glossy illustrations such as the figure above from William Harvey’s experiments on the circulation of blood. That alone is worth picking up a copy of this one.&lt;/p&gt;

&lt;h2 id=&quot;attack-of-the-50-foot-blockchain&quot;&gt;Attack of the 50 Foot Blockchain&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;https://rocknerd.co.uk/wp-content/uploads/2017/07/botandguy.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Author: David Gerard&lt;/p&gt;

&lt;p&gt;Published: 2017&lt;/p&gt;

&lt;p&gt;This book discusses both the origins and prospects of Bitcoin and other cryptocurrencies and blockchain as a product. Rather than being a dry, technical account of the software behind the scenes, Gerard focuses on the actual implementations and the ideologies of those involved. The writing style is comical and conversational, and the author pulls no punches in throwing cold water on the cryptocurrency hype. The two quotes below are illustrative:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Bitcoin community is now sufficiently dysfunctional that even such a simple proposal as “OK, let’s increase the block size to two megabytes” led to community schisms, code forks, retributive DDOS attacks, death threats, a split between Chinese miners and American core programmers … and plenty of other clear evidence that this and other problems in the Bitcoin protocol will never be fixed by a consensus process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Bitcoins are so hard for normal people to use that from CryptoLocker on, ransomware operators have been known to provide technical support to victims, so they can work out how to pay them and unlock their files. F-Secure even compiled a customer service evaluation of ransomware gangs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Gerard’s take is that cryptocurrencies have not delivered on their promises. The implementations of Bitcoin and Ethereum have been fraught with troubles, e.g., &lt;a href=&quot;https://www.wired.com/2016/06/50-million-hack-just-showed-dao-human/&quot;&gt;The DAO hack&lt;/a&gt;. The technologies are not easy to use, and the primary use case seems to be some form of money laundering. As for blockchain, there have been successful distributed ledgers, e.g. Git (which I use in conjunction with GitHub to host this site); however, for most use cases, it is overengineering.&lt;/p&gt;

&lt;p&gt;I picked up this book after three months into seriously learning about and eventually mining cryptocurrencies. Keep in mind that this was in the heat of the Bitcoin fever in December 2017! Outside of an &lt;a href=&quot;https://hackernoon.com/ten-years-in-nobody-has-come-up-with-a-use-case-for-blockchain-ee98c180100&quot;&gt;article on Hackernoon&lt;/a&gt;, many of the resources I came across were highly optimistic, e.g., &lt;a href=&quot;https://www.coursera.org/learn/cryptocurrency/home/info&quot;&gt;the &lt;em&gt;Bitcoin and Cryptocurrency Technologies&lt;/em&gt; Coursera course&lt;/a&gt;. Gerard’s book lent a good bit of perspective to my enthusiasm. I look forward to looking back at this one in a decade or so.&lt;/p&gt;

&lt;p&gt;You can read more on &lt;a href=&quot;https://davidgerard.co.uk/blockchain/&quot;&gt;David Gerard’s blog&lt;/a&gt; of the same name.&lt;/p&gt;

&lt;h2 id=&quot;exact-thinking-in-demented-times&quot;&gt;Exact Thinking in Demented Times&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Karl Sigmund&lt;/p&gt;

&lt;p&gt;Published: 2017&lt;/p&gt;

&lt;p&gt;This book chronicles the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vienna_Circle&quot;&gt;Vienna Circle&lt;/a&gt; (or Wiener Kreis), a motley collection of mathematicians, philosophers, and scientists who regularly met in interwar Vienna. The aim of the circle was to orient philosophy or foundations on a scientific basis to reflect developments in mathematics, logic, and the natural sciences and expunge most of traditional metaphysics and moral philosophy, a view referred to as logical positivism.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;30%&quot; height=&quot;30%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/1/17/Schlick_sitting.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rather than focusing primarily on the evolution of logical positivism as an idea, Sigmund focuses on the lives and relations between members of the circle. While this account can come off as a bit gossipy, it is instructive in understanding the evolution of an exciting new research program composed of independent thinkers with political differences and big personalities. The Circle was chaired by Moritz Schlick (pictured to the right), a professor of inductive sciences at the University of Vienna. Other members of the circle featured include Rudolph Carnap, Otto Neurath, Phillip Frank, Viktor Kraft, and Hans Hahn.&lt;/p&gt;

&lt;p&gt;The author captures the excitement of the times through detailing the dissemination of the Circle’s research through a series of international conferences, a controversial manifesto (both internally and externally) called &lt;em&gt;The Scientific Conception of the World: The Vienna Circle&lt;/em&gt; (&lt;em&gt;Wissenschafteliche Weltauffassung: Der Wiener Kreis&lt;/em&gt;), an edited journal &lt;a href=&quot;https://www.springer.com/philosophy/journal/10670/PS2&quot;&gt;&lt;em&gt;Erkenntnis&lt;/em&gt;&lt;/a&gt; which is active today, and several published monographs.&lt;/p&gt;

&lt;p&gt;This excitement is best captured in A.J. Ayer’s 1936 report on the developments by the Vienna Circle from a visit in 1933: &lt;a href=&quot;https://en.wikipedia.org/wiki/Language,_Truth,_and_Logic&quot;&gt;Language, Truth, &amp;amp; Logic&lt;/a&gt;. My introduction to logical positivism was through Ayer’s book in a history of analytic philosophy seminar, and my continued interest in the Vienna Circle stems from their ability to have made influential contributions across many disciplines (particularly those I’m interested in) often regarded as requiring high specialization.&lt;/p&gt;

&lt;p&gt;I recommend this book as a companion to &lt;a href=&quot;https://www.harpercollins.com/9780060936648/wittgensteins-poker&quot;&gt;Wittgenstein’s Poker&lt;/a&gt;, the story of the encounter between two thinkers, Karl Popper and Ludwig Wittgenstein, each at the periphery of the Vienna Circle.&lt;/p&gt;</content><author><name></name></author><summary type="html">Here are the top three books I’ve read in 2018. They are presented below in chronological order. While these three books seem rather disparate, they are bound together by themes of innovation, conflict, and ideology.</summary></entry></feed>