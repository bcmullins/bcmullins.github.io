<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-12-11T23:14:10-06:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Brett Mullins</title><subtitle>Researcher - Data Scientist</subtitle><entry><title type="html">Top Articles I’ve Read in 2019</title><link href="http://localhost:4000/Top-Articles-2019/" rel="alternate" type="text/html" title="Top Articles I've Read in 2019" /><published>2019-12-11T00:00:00-06:00</published><updated>2019-12-11T00:00:00-06:00</updated><id>http://localhost:4000/Top-Articles-2019</id><content type="html" xml:base="http://localhost:4000/Top-Articles-2019/">&lt;p&gt;Below are the top eleven articles I’ve read in 2019. A theme of methodology runs through this set of papers, especially statistical methodology. There’s also some fun miscellany mixed in with blockchain (whose craze seems like a lifetime ago now), unicorns, and the history of the English language. To my surprise, all of these articles are from the present decade. They are presented in chronological order.&lt;/p&gt;

&lt;h2 id=&quot;the-credibility-revolution-in-empirical-economics-how-better-research-design-is-taking-the-con-out-of-econometrics&quot;&gt;The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con out of Econometrics&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Joshua D. Angrist and Jörn-Steffen Pischke&lt;/p&gt;

&lt;p&gt;Publication: Journal of Economic Perspectives&lt;/p&gt;

&lt;p&gt;Published: Spring 2010&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1257/jep.24.2.3&quot;&gt;10.1257/jep.24.2.3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper is in some sense a response to Ed Leamer’s 1983 &lt;a href=&quot;https://www.jstor.org/stable/1803924&quot;&gt;Let’s Take the Con Out of Econometrics&lt;/a&gt; that questioned the methods of econometrics both in theory and in practice for parading as objective science when in reality several hidden judgements are made by practitioners. Angrist and Pischke argue that, since Leamer’s critique, a focus on research design, i.e., the way in which studies are formulated and data is collected, has led to a revolution in econometrics.  Specifically, the use of quasi-experimental methods such as instrumental variables, regression discontinuity design, and difference-in-differences has led to more robust estimates of causal effects. Whereas Leamer proposed sensitivity analysis as a solution, the authors contend that better design better addresses Leamer’s worries.&lt;/p&gt;

&lt;figure style=&quot;float: right; display: inline-block&quot;&gt;
  &lt;img src=&quot;http://localhost:4000/images/blog/top_articles_2019/rdd.png&quot; width=&quot;350&quot; height=&quot;275&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://davidevans.blog/2018/05/18/activity-for-teaching-regression-discontinuity-design/&quot;&gt;Example Regression Discontinuity Design&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Angrist and Pischke give several examples with a focus on applied microeconomics, specifically labor and education, since these fields adapt easily to the quasi-experimental methodology. This paper sheds light on the current practice of econometrics, or at least how one influential party sees it. The authors remark that these methods may not be sexy or directly and quickly get at the “big” questions, but the quasi-experimental approach to econometrics proceeds by accumulation and small gains in understanding, similar to how we think about the hard sciences.&lt;/p&gt;

&lt;p&gt;This paper gives one a nice introduction to the ideas and context of Angrist and Pischke’s text &lt;a href=&quot;https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics&quot;&gt;Mostly Harmless Econometrics&lt;/a&gt; or their more friendly &lt;a href=&quot;https://www.masteringmetrics.com&quot;&gt;Mastering ‘Metrics&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;english-is-not-normal&quot;&gt;English is not normal&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: John McWhorter&lt;/p&gt;

&lt;p&gt;Publication: Aeon&lt;/p&gt;

&lt;p&gt;Published: Nov 2015&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://aeon.co/essays/why-is-english-so-weirdly-different-from-other-languages&quot;&gt;aeon.co/essays/why-is-english-so-weirdly-different-from-other-languages&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is a quirky essay about the history of the English language. The author argues its unique place among languages is largely an artifact of history. Here are two interesting tidbits. For the first, Old English is basically unrecognizable from contemporary English; witness the following: &lt;em&gt;“Hwæt, we gardena in geardagum þeodcyninga þrym gefrunon”&lt;/em&gt; is Old English for “So, we Spear-Danes have heard of the tribe-kings’ glory in days of yore”. While that sentence seems a bit contrived, you get the point. For the second tidbit, over time, English adopted French and Latin words along with a natural scale of refinement and formality: for example, kingly (English), royal (French), regal (Latin) all denote the same idea but express it with increasing regard.&lt;/p&gt;

&lt;p&gt;The author leaves us with this summary:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“The common idea that English dominates the world because it is ‘flexible’ implies that there have been languages that failed to catch on beyond their tribe because they were mysteriously rigid. I am not aware of any such languages. What English does have on other tongues is that it is deeply peculiar in the structural sense. And it became peculiar because of the slings and arrows – as well as caprices – of outrageous history.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-need-for-cognitive-science-in-methodology&quot;&gt;The Need for Cognitive Science in Methodology&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Sander Greenland&lt;/p&gt;

&lt;p&gt;Publication: American Journal of Epidemiology&lt;/p&gt;

&lt;p&gt;Published: Aug 2017&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1093/aje/kwx259&quot;&gt;10.1093/aje/kwx259&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There has been much recent literature on the deficiency of current statistical methods; however, not much attention has been paid to cognitive biases of practitioners using these methods. Greenland describes three such biases:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) dichotomania, the compulsion to perceive quantities as dichotomous even when dichotomization is unnecessary and misleading, as in inferences based on whether a P value is “statistically significant”; 2) nullism, the tendency to privilege the hypothesis of no difference or no effect when there is no scientific basis for doing so, as when testing only the null hypothesis; and 3) statistical reification, treating hypothetical data distributions and statistical models as if they reflect known physical laws rather than speculative assumptions for thought experiments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While these points have been brought up by &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&quot;&gt;Gelman&lt;/a&gt; and others in some form, Greenland provides a cohesive and convincing account of when it is that the tools themselves aren’t the (only) problem. He argues in favor of the methodological position that “Statistical analyses are merely thought experiments, informing us as to what would follow deductively under their assumptions” (&lt;a href=&quot;https://statmodeling.stat.columbia.edu/2019/12/03/whats-wrong-with-bayes/&quot;&gt;Gelman agrees&lt;/a&gt;). While an interesting philosophical position (and one have sympathies for), this view certainly doesn’t match my experience with how practitioners treat statistical analyses in the wild. Perhaps, that’s additional evidence that we ought to be thinking about the cognitive status of practitioners.&lt;/p&gt;

&lt;p&gt;Amusing quotes are peppered throughout: “This null obsession is the most destructive pseudoscientific gift that conventional statistics (both frequentist and Bayesian) has given the modern world.”&lt;/p&gt;

&lt;p&gt;Greenland also takes a swipe at &lt;a href=&quot;https://plato.stanford.edu/entries/learning-formal/&quot;&gt;formal methodology&lt;/a&gt; and &lt;a href=&quot;https://plato.stanford.edu/entries/pseudo-science/#KarPop&quot;&gt;philosophy of science&lt;/a&gt;: “thinking that parsimony is a property of nature when it is instead only an effective learning heuristic, or that refutationism involves believing hypotheses until they are falsified, when instead it involves never asserting a hypothesis is true”. While I buy much of what Greenland has to say, I may be too much of an ideologue to go this far.&lt;/p&gt;

&lt;h2 id=&quot;blockchain-is-not-only-crappy-technology-but-a-bad-vision-for-the-future&quot;&gt;Blockchain is not only crappy technology but a bad vision for the future&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Kai Stinchcombe&lt;/p&gt;

&lt;p&gt;Published: Apr 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec&quot;&gt;medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec&lt;/a&gt;&lt;/p&gt;

&lt;figure style=&quot;float: right; display: inline-block&quot;&gt;
  &lt;img src=&quot;http://localhost:4000/images/blog/top_articles_2019/blockchainmail.jpg&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center&quot;&gt;Blockchainmail&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This is a followup to &lt;a href=&quot;https://hackernoon.com/ten-years-in-nobody-has-come-up-with-a-use-case-for-blockchain-ee98c180100&quot;&gt;Ten years in, nobody has come up with a use for blockchain&lt;/a&gt;. The author argues two points: people often confuse blockchain the idea with blockchain the technology; and we actually do not want fully trustless transactions. On the first point, people want a way for correct information to be entered into a tamperproof database, e.g., tracking fruit to ensure it’s organic; however, nothing stops bad data from being entered into the database. On the second point, the author defends the value of social trust and points out examples where even major uses of cryptocurrencies, e.g., the Silk Road and Ripple, rely on trust measures, e.g., seller reviews, rather than smart contracts or anything else of the sort. These social institutions are better than having to trust the code of someone offering a smart contract or from having to write one’s contract every time one wishes to make a transaction. Trading this trust for other ends leads to a world that’s “not a paradise but a crypto-medieval hellhole.”&lt;/p&gt;

&lt;h2 id=&quot;why-data-is-never-raw&quot;&gt;Why Data is Never Raw&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Nick Barrowman&lt;/p&gt;

&lt;p&gt;Publication: The New Atlantis&lt;/p&gt;

&lt;p&gt;Published: Summer/Fall 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.thenewatlantis.com/publications/why-data-is-never-raw&quot;&gt;www.thenewatlantis.com/publications/why-data-is-never-raw&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article applies the idea of &lt;a href=&quot;https://plato.stanford.edu/entries/science-theory-observation/#HowObsEviMigTheLad&quot;&gt;theory-ladeness of observation&lt;/a&gt; from the philosophy of science to the idea of raw data in statistics and data science. Just as the presuppositions of an observer affect how one interprets what is observed, how data are captured (not just given) affect what the data say.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“For example, rates of domestic violence were historically underestimated because these crimes were rarely documented. Polling data may miss people who are homeless or institutionalized, and if marginalized people are incompletely represented by opinion polls, the results may be skewed. Data sets often preferentially include people who are more easily reached or more likely to respond.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For this reason, the method of data acquisition is highly relevant to its interpretation and use whether for survey data from such and such a population, measurements in a lab, or transaction logs from a production system. To sum up, the author offers the words of Geoffrey Bowker of UC Irvine: “Raw data is both an oxymoron and a bad idea; to the contrary, data should be cooked with care.”&lt;/p&gt;

&lt;h2 id=&quot;the-bias-bias-in-behavioral-economics&quot;&gt;The Bias Bias in Behavioral Economics&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Gerd Gigerenzer&lt;/p&gt;

&lt;p&gt;Publication: Review of Behavioral Economics&lt;/p&gt;

&lt;p&gt;Published: Dec 2018&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1561/105.00000092&quot;&gt;10.1561/105.00000092&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article is an attack on the current state of behavioral economics. Since the work of Kahneman and Tverksy in the 1970s, behavioral economics has largely focused on the identification of &lt;a href=&quot;https://hbr.org/2015/05/from-economic-man-to-behavioral-economics&quot;&gt;biases and heuristics for decision making&lt;/a&gt;. Biases are consistent and widespread departures from the rational choice model. The author argues for the bias bias: attributing a bias to behavior that is potentially well-motivated.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;40%&quot; height=&quot;50%&quot; src=&quot;/images/blog/top_articles_2019/coin_flip.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most interesting bias discussed is the law of small numbers: “People think a string is more likely the closer the number of heads and tails corresponds to the underlying equal probabilities. For instance, the string HHHHHT is deemed more likely than HHHHHH.” When we consider six coin flips, this intuition is clearly incorrect since each possible outcome is equally likely and the events are independent. Consider, instead, if there are seven coin flips. We still have that all outcomes are equally likely; however, HHHHHT appears as a substring in four outcomes, while HHHHHH only appears in three. This result holds for any number of coin flips greater than the length of the substring. This potentially explains people’s otherwise confused intuitions.&lt;/p&gt;

&lt;p&gt;The paper concludes by suggesting that behavioral economics abandon its current research program and follow the alternative &lt;a href=&quot;https://plato.stanford.edu/entries/bounded-rationality/&quot;&gt;bounded rationality&lt;/a&gt; vision set out by Herbert Simon in the latter half of the 20th century. Rather than focusing on departures from the expected utility model, Simon focused more on descriptive models and included notions of choice under uncertainty rather than exclusively choice under risk.&lt;/p&gt;

&lt;h2 id=&quot;intellectual-control&quot;&gt;Intellectual Control&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: George Fairbanks&lt;/p&gt;

&lt;p&gt;Publication: IEEE Software&lt;/p&gt;

&lt;p&gt;Published: Jan/Feb 2019&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;http://dx.doi.org/10.1109/MS.2018.2874294&quot;&gt;10.1109/MS.2018.2874294&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article is about the notion of control in software development. The current practice of &lt;a href=&quot;https://en.wikipedia.org/wiki/Test-driven_development&quot;&gt;test driven development&lt;/a&gt; provides statistical control, i.e., if all of the test cases pass, then there likely is not a problem. In contrast, an alternative notion is intellectual control, i.e., being able to explain what the code/program/system is doing. There are obvious tradeoffs: intellectual control doesn’t scale well; statistical control can be effective if executed well; etc.&lt;/p&gt;

&lt;p&gt;The author provides an effective analogy that differentiates the two notions of control:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“imagine driving a car on a road with guardrails. In this metaphor, the guardrails are tests and driving the car is us writing programs. He then wondered if it’s OK to successfully arrive at our destination after hitting the guardrails during the journey. His audience laughed because having a car under control means we are able to drive without hitting the rails.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tony Hoare said it best in &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=358549.358561&quot;&gt;his Turing Award lecture&lt;/a&gt;, “There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies.”&lt;/p&gt;

&lt;h2 id=&quot;unicorns-cheshire-cats-and-the-new-dilemmas-of-entrepreneurial-finance&quot;&gt;Unicorns, Cheshire cats, and the new dilemmas of entrepreneurial finance&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Martin Kenney and John Zysman&lt;/p&gt;

&lt;p&gt;Publication: Venture Capital&lt;/p&gt;

&lt;p&gt;Published: Mar 2019&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1080/13691066.2018.1517430&quot;&gt;10.1080/13691066.2018.1517430&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper examines the rise of the unicorn: a start-up with a private valuation of over one billion USD. Many of these unicorns came about due to the low barrier to entry of starting a firm and raising capital. Often these firms are technology-focused with lots of intangible assets, e.g., Uber, AirBnB, etc., and are, thus, easily scalable. To scale up and gain market share, these companies often burn through massive amounts of capital while in a growth-phase. An ecosystem of funding, including traditional VC, angel investors, incubators, etc., has developed to progress these firms through their growth-phase with the eventual goal of selling their stake to another investor or to the public.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;60%&quot; height=&quot;60%&quot; src=&quot;/images/blog/top_articles_2019/cheshire.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A result is that incumbent firms in traditionally stable industries have been “disrupted” by growth-phase start-ups aiming to take market share. The attempt to capture market share is often done by offering lower prices, e.g., an Uber or Lyft ride is usually cheaper than getting a taxi. At the same time, the start-up is using capital money to fund this behavior. The fight for market share is motivated by the winner-takes-all nature of many industries, since there is not necessarily an market need for several search engines, social media platforms, etc.&lt;/p&gt;

&lt;p&gt;From an economic perspective, it is not clear if this method of producing firms is beneficial overall, since the goal for investors appears to be to raise the private valuation to sell to another investor or the public without concern for the long-term stability of the start-up.&lt;/p&gt;

&lt;p&gt;The authors provide a nice summary for their paper’s goal: “to interrogate the enthusiasm for backing entrepreneurial start-ups, losses or not, and for seeking to turbo-charge their growth to the point that they become the so-called ‘unicorns.’”&lt;/p&gt;

&lt;p&gt;This paper shared much in common with &lt;a href=&quot;https://press.princeton.edu/books/hardcover/9780691175034/capitalism-without-capital&quot;&gt;Capitalism without Capital&lt;/a&gt; which made the &lt;a href=&quot;## TODO&quot;&gt;2019 Top Books list&lt;/a&gt;. I came across this paper from &lt;a href=&quot;https://www.economist.com/briefing/2019/04/17/the-wave-of-unicorn-ipos-reveals-silicon-valleys-groupthink&quot;&gt;an article in The Economist&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-new-americanism&quot;&gt;A New Americanism&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Jill Lepore&lt;/p&gt;

&lt;p&gt;Publication: Foreign Affairs&lt;/p&gt;

&lt;p&gt;Published: Mar/Apr 2019&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.foreignaffairs.com/articles/united-states/2019-02-05/new-americanism-nationalism-jill-lepore&quot;&gt;www.foreignaffairs.com/articles/united-states/2019-02-05/new-americanism-nationalism-jill-lepore&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This short article argues that nations look to an origin story to explain their existence and that this story changes with the times. It wasn’t until the 1840s that the people of the United States viewed themselves as a nation (not sure what population this applies to). These shared myths continued to be developed until the second half of the twentieth century where they fell out of fashion for historians as nationalism yielded way to globalization, first politically then economically.&lt;/p&gt;

&lt;p&gt;Just because academic trends changed does not entail that the nation no longer looks for its founding myth; rather, these myths are just not provided by historians. The author alludes to this being the origin of the nativist, so-called nationalist movement observed in the US as well as in other countries. The author urges historians to write about the nation once more.&lt;/p&gt;

&lt;p&gt;Lepore’s history of the United States &lt;a href=&quot;https://www.thesetruthsbook.com&quot;&gt;These Truths&lt;/a&gt; was released in late 2018.&lt;/p&gt;

&lt;h2 id=&quot;the-dangers-of-post-hoc-interpretability-unjustified-counterfactual-explanations&quot;&gt;The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki&lt;/p&gt;

&lt;p&gt;Publication: Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)&lt;/p&gt;

&lt;p&gt;Published: Jul 2019&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.ijcai.org/proceedings/2019/0388.pdf&quot;&gt;ijcai.org/proceedings/2019/0388.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper extends the idea of a local explanation of a classifier’s prediction to counterfactually-justified local explanations to provide a link between the classifier and the training data. For an explanation to be justified for a given point in the feature space $x$, there must exist point $z$ in the training data, perhaps the nearest point to $x$, that’s correctly classified where $x, z$ belong to the same label and are topologically &lt;a href=&quot;https://en.wikipedia.org/wiki/Connected_space#Path_connectedness&quot;&gt;path connected&lt;/a&gt; with respect to the euclidean topology on the feature space. The intuition is that a local explanation for a prediction cannot be justified in this stronger sense if the point in question is isolated from correctly predicted instances in the training data. This leads to the amusing corollary that no explanations are justified if the predictive model classifies all instances in the training data incorrectly.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/images/blog/top_articles_2019/counterfactual_explanations.png&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center&quot;&gt;Left: explanation not justified; right: explanation justified&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;The authors operationalize this through an approximation algorithm to provide an estimate of whether or not a point is path connected to a correct prediction of an observation in the training data sharing the same label. A &lt;a href=&quot;https://www.ecmlpkdd2019.org/downloads/paper/226.pdf&quot;&gt;followup paper&lt;/a&gt; by this group appeared at KDD in August.&lt;/p&gt;

&lt;p&gt;I find this paper particularly compelling since it connects closely with my work studying the &lt;a href=&quot;https://arxiv.org/abs/1910.08595v2&quot;&gt;topological properties of explanations of classifier predictions&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;mistaken-i-think-therefore-i-make-mistakes-and-change-my-mind&quot;&gt;Mistaken: I think, therefore I make mistakes and change my mind&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Daniel Ward&lt;/p&gt;

&lt;p&gt;Publication: Aeon&lt;/p&gt;

&lt;p&gt;Published: Oct 2019&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://aeon.co/essays/i-think-therefore-i-make-mistakes-and-change-my-mind&quot;&gt;aeon.co/essays/i-think-therefore-i-make-mistakes-and-change-my-mind&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article is about the notion of human infallibility and how it is antithetical to rationality. On this conception, being rational is being able to give reasons for one’s actions and that involves the possibility of error. The author argues that the notion of human infallibility underlies both &lt;a href=&quot;https://plato.stanford.edu/entries/moral-relativism/&quot;&gt;moral relativism&lt;/a&gt; in anthropology/sociology and &lt;a href=&quot;https://en.wikipedia.org/wiki/Revealed_preference&quot;&gt;revealed preference&lt;/a&gt; in economics, both of which the author finds objectionable.&lt;/p&gt;

&lt;p&gt;The case of revealed preference is the more interesting of the two. Revealed preference is the notion that an agent reveals their preferences for goods through their choices, a behaviorist approach to choice/decision theory. The author argues that this theory does not account for genuine mistakes. We may think that is because the agent in the model is more rational than humans; however, this turns on the specific notion of rationality. On the reason giving account, mistakes are the hallmark of rationality.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“To err is human. Missteps, misapprehensions, misspeakings, momentary lapses and mess-ups are part of the fabric of life. Yet we are capable of making mistakes precisely because we are thoughtful, intelligent beings with complex goals and sincerely held values. We wouldn’t be able to if we were otherwise. Regrets: we’ve had a few. But we are the wiser for them.”&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">Below are the top eleven articles I’ve read in 2019. A theme of methodology runs through this set of papers, especially statistical methodology. There’s also some fun miscellany mixed in with blockchain (whose craze seems like a lifetime ago now), unicorns, and the history of the English language. To my surprise, all of these articles are from the present decade. They are presented in chronological order.</summary></entry><entry><title type="html">Introducing mobilityIndexR, an R package for measuring mobility</title><link href="http://localhost:4000/mobilityindexr/" rel="alternate" type="text/html" title="Introducing mobilityIndexR, an R package for measuring mobility" /><published>2019-10-16T00:00:00-05:00</published><updated>2019-10-16T00:00:00-05:00</updated><id>http://localhost:4000/mobilityindexr</id><content type="html" xml:base="http://localhost:4000/mobilityindexr/">&lt;p&gt;[Intro]&lt;/p&gt;

&lt;h2 id=&quot;simple-example&quot;&gt;Simple Example&lt;/h2&gt;

&lt;h2 id=&quot;advanced-features&quot;&gt;Advanced Features&lt;/h2&gt;

&lt;h2 id=&quot;future-features&quot;&gt;Future Features&lt;/h2&gt;</content><author><name></name></author><summary type="html">[Intro]</summary></entry><entry><title type="html">Top Books I’ve Read in 2019</title><link href="http://localhost:4000/Top-Books-2019/" rel="alternate" type="text/html" title="Top Books I've Read in 2019" /><published>2019-10-16T00:00:00-05:00</published><updated>2019-10-16T00:00:00-05:00</updated><id>http://localhost:4000/Top-Books-2019</id><content type="html" xml:base="http://localhost:4000/Top-Books-2019/">&lt;p&gt;[Intro]&lt;/p&gt;

&lt;h2 id=&quot;measure-and-category-a-survey-of-the-analogies-between-topological-and-measure-spaces&quot;&gt;Measure and Category: A Survey of the Analogies between Topological and Measure Spaces&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: John C. Oxtoby&lt;/p&gt;

&lt;p&gt;Published: 1971 (1980 2nd ed.)&lt;/p&gt;

&lt;h2 id=&quot;the-structures-of-everyday-life-civilization-and-capital-15th---18th-century-volume-1&quot;&gt;The Structures of Everyday Life: Civilization and Capital, 15th - 18th Century: Volume 1&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Fernand Braudel&lt;/p&gt;

&lt;p&gt;Published: 1981&lt;/p&gt;

&lt;p&gt;Braudel’s book paints a picture of everyday life in Europe during the 14th to the 18th centuries. Rather than focus on the lives and happenings of so-called “great men”, Braudel discusses the minutiae that is easily overlooked. These range from the evolution of food and drink to fashion, housing, and decor, to manners, money, and technology.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;60%&quot; height=&quot;60%&quot; src=&quot;/images/blog/top_books_2019/hoe_cultivation_belt.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most fruitful way to understand this book is as a primer on the era that sets the stage for the next two volumes of the work. As the goal is to provide the reader with a vivid context, Braudel is not regularly ventures outside of Europe and into the past to tell the story of how the state of affairs played out and what others were doing at the same time.&lt;/p&gt;

&lt;p&gt;Braudel can be somewhat uneven as he jumps between topics. For instance, the book begins with a long (perhaps) 70 page digression on the statistics world population/demography from the late Middle Ages to the 20th century. It is apparent that Braudel is interested in the Chinese, providing the foil to countless comparisons with Europe.&lt;/p&gt;

&lt;p&gt;The author does venture into big questions asking why it is that Europeans and not the Chinese initiated the age of explorations, despite the head start of the Chinese. While these questions are undoubtedly interesting and worthwhile, this book shines in the details. I would be remiss not to mention that throughout there are numerous beautiful color illustrations of paintings and documents.&lt;/p&gt;

&lt;p&gt;This is a book where even if one takes nothing specific away, one will take away much context. The &lt;a href=&quot;https://www.binghamton.edu/fbc/&quot;&gt;Fernand Braudel Center at Binghamton University&lt;/a&gt; continues social science research in Braudel’s tradition.&lt;/p&gt;

&lt;h2 id=&quot;capitalism-without-capital&quot;&gt;Capitalism without Capital&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Jonathan Haskel and Stian Weslake&lt;/p&gt;

&lt;p&gt;Published: 2018&lt;/p&gt;</content><author><name></name></author><summary type="html">[Intro]</summary></entry><entry><title type="html">Resources for Learning Measure Theory</title><link href="http://localhost:4000/measure_theory_resources/" rel="alternate" type="text/html" title="Resources for Learning Measure Theory" /><published>2019-10-13T00:00:00-05:00</published><updated>2019-10-13T00:00:00-05:00</updated><id>http://localhost:4000/measure_theory_resources</id><content type="html" xml:base="http://localhost:4000/measure_theory_resources/">&lt;p&gt;When approaching measure theory for the first time, the ideas can seem opaque and unmotivated. This is amplified since many students of measure theory are not coming from a strictly mathematics background and may be approaching the material on their own outside of the classroom. In addition to first-year math graduate students and advanced math undergraduates, students in stats, economics, the hard sciences, etc. will find their way into learning measure theory. This is a guide to resources for learning measure theory that tries to keep in mind that many (myself included) approach the material with an atypical background.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;To work through the material for measure theory, one should be familiar with the concepts and basic results from real analysis (limit, sup/inf, metric, etc.) and have some exposure to general topology (open sets, closed sets, continuity, etc.). While not strictly necessary depending on one’s background, it’s helpful to have exposure to both the differential calculus and probability to have mental models available to check against these measure-theoretic ideas. Francis Su provides an excellent &lt;a href=&quot;https://analysisyawp.blogspot.com/2013/01/lectures.html&quot;&gt;set of video lectures&lt;/a&gt; to get one up to speed on real analysis. As for the elements of topology, I suggest taking a look at the book &lt;a href=&quot;http://www.topologywithouttears.net&quot;&gt;&lt;em&gt;Topology without Tears&lt;/em&gt;&lt;/a&gt; by Sidney A. Morris, available for free in pdf form.&lt;/p&gt;

&lt;h2 id=&quot;books&quot;&gt;Books&lt;/h2&gt;

&lt;p&gt;I recommend two texts: Richard Bass’ &lt;em&gt;Real Analysis for Graduate Students&lt;/em&gt; as the primary text and David Bressoud’s &lt;em&gt;A Radical Approach to Lebesgue’s Theory of Integration&lt;/em&gt; as a supplement.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;http://bass.math.uconn.edu/real-cover.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there are stacks upon stacks of measure theory texts available, the Bass book combines instructive proofs with an organization that motivates the material and a collection of interesting and (sometimes) difficult exercises. These more difficult exercises will be particularly useful for those studying for Comps/Quals. The selling point is that all versions of the book are available in pdf form on the &lt;a href=&quot;http://bass.math.uconn.edu/real.html&quot;&gt;author’s website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A typical course in measure theory will take one through chapter fifteen. This starts with the definition of a measure on sets (1-4) to a measure on a function (5) to integration and differentiation of functions (6-14) and, finally, to $\mathcal{L}_p$ spaces of functions (15). The Bass book includes chapters on topology (20) and measure-theoretic probability (21) for foundations and applications, respectively; however, these sections are not as well put-together as the first half of the book. With that being said, this book contains everything one needs to get a handle on measure theory in a reasonably digestible form.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: left; display: inline-block; margin: 0px 20px 0px 0px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://www.macalester.edu/~bressoud/books/ARattleToy.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the Bass book contains the meat of a course on measure theory, it often lacks the context one may find in a classroom discussion. Bressoud’s &lt;a href=&quot;https://www.cambridge.org/us/academic/subjects/mathematics/abstract-analysis/radical-approach-lebesgues-theory-integration?format=PB&amp;amp;isbn=9780521711838&quot;&gt;&lt;em&gt;A Radical Approach to Lebesgue’s Theory of Integration&lt;/em&gt;&lt;/a&gt; approaches the Lebesgue integral and measure theory more generally from a historical perspective. This book need not be read cover-to-cover; I’ve found it is more useful to skip around to relevant sections. I &lt;em&gt;do&lt;/em&gt; recommend reading chapters three, four, and five on the current conception of $\mathbb{R}$, the problem posed to Riemann integration by nowhere dense sets, and the development of early measure theory.&lt;/p&gt;

&lt;h2 id=&quot;video-lectures&quot;&gt;Video Lectures&lt;/h2&gt;

&lt;p&gt;It can often be difficult to learn streamlined material developed for a first-year graduate course outside the context of that course. For some people (myself included), hearing and seeing someone walkthrough an example can be the difference between internalizing the example to build intuition and second-guessing the result each time a similar case comes up.&lt;/p&gt;

&lt;p&gt;If this sounds like you, the &lt;a href=&quot;https://www.youtube.com/playlist?list=PLo4jXE-LdDTQq8ZyA8F8reSQHej3F6RFX&quot;&gt;collection of measure theory lectures&lt;/a&gt; from &lt;a href=&quot;http://w3.impa.br/~landim/&quot;&gt;Claudio Landim&lt;/a&gt; will be a great help! Landim’s lectures provide a comprehensive measure theory course that captures the feel of a classroom with over 30 hours of material. Landim is particularly good with providing insightful examples and helping the viewer focus on the key steps in proofs.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?!&lt;/h2&gt;

&lt;p&gt;Getting a handle on the basics of measure theory allows one pursue numerous areas of mathematics and its applications. Below are some that I find interesting and recommendations for a first dive into each area.&lt;/p&gt;

&lt;h3 id=&quot;measure-theoretic-probability&quot;&gt;Measure-theoretic Probability&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;http://probability.ca/jeff/grprobcov2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Measure theory is the most common foundation for a rigorous treatment of probability. Many of the odd rules one sees in an initial treatment of probability are reduced to questions of measure theory. For instance, rather than having a rule or heuristic saying that the probability of a continuous random variable taking the value of a single point is zero, we can observe that a probability measure with a continuous density function is absolutely continuous to the Lebesgue measure and that a single point is a Lebesgue null or measure zero set.&lt;/p&gt;

&lt;p&gt;For this, I recommend Jeffrey Rosenthal’s &lt;a href=&quot;http://probability.ca/jeff/grprobbook.html&quot;&gt;A First Look at Rigorous Probability Theory&lt;/a&gt;. This book also opens up paths to explore in financial mathematics, stochastic process models, etc.&lt;/p&gt;

&lt;h3 id=&quot;topology&quot;&gt;Topology&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://www.pearsonhighered.com/assets/bigcovers/0/1/3/1/0131816292.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In typical treatments of measure theory, one begins by working with the Lebesgue measure in euclidean space and then generalizes to measures on other sorts of spaces, e.g., the counting measure on the space formed by subsets of the natural numbers. Though more abstract, these spaces are but one sort of topological space. Point-set topology is the study of properties of and between general topological spaces.&lt;/p&gt;

&lt;p&gt;The recommended text in this area is the first half of James Munkres’ &lt;a href=&quot;https://www.pearson.com/us/higher-education/program/Munkres-Topology-2nd-Edition/PGM56881.html&quot;&gt;&lt;em&gt;Topology&lt;/em&gt;&lt;/a&gt;. The second half of this book devoted to Algebraic Topology, a different perspective on studying topological spaces; however, better treatments exist of this latter topic. While Munkres can be terse at times, his dark humor is undeniable: see the exposition beginning Section 33 on the Urysohn Lemma. I read it as humor anyhow.&lt;/p&gt;

&lt;h3 id=&quot;measure-theory-meets-topology&quot;&gt;Measure Theory meets Topology&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://images.springer.com/sgw/books/medium/9780387905082.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In measure theory, we have a notion of small or negligible sets: null sets. A corresponding notion of small or negligible sets in a topological space is that of being &lt;a href=&quot;https://en.wikipedia.org/wiki/Meagre_set&quot;&gt;meagre&lt;/a&gt;, i.e., the countable union of nowhere dense sets. John Oxtoby’s &lt;a href=&quot;https://www.springer.com/gp/book/9780387905082&quot;&gt;&lt;em&gt;Measure and Category&lt;/em&gt;&lt;/a&gt; studies analogies between these two concepts and proves “duality results” on when we can interchange meagre for null sets (and vice versa) in theorems.&lt;/p&gt;

&lt;p&gt;One aspect of meagre sets being negligible is that they contain no non-trivial open subsets. One may be tempted to think that the meagre sets are just the null sets on the reals; however, &lt;a href=&quot;https://mathoverflow.net/questions/43478/is-there-a-measure-zero-set-which-isnt-meagre&quot;&gt;this is not the case&lt;/a&gt;. We can partition $\mathbb{R}$ into a null set and a meagre set of low Borel rank.&lt;/p&gt;

&lt;h3 id=&quot;descriptive-set-theory&quot;&gt;Descriptive Set Theory&lt;/h3&gt;

&lt;p&gt;Descriptive set theory studies the structure and properties of “well-behaved” subsets of the reals and similar but more general spaces called &lt;a href=&quot;https://en.wikipedia.org/wiki/Polish_space&quot;&gt;polish spaces&lt;/a&gt;. One starting place is the structure of the Borel sets, i.e., the closure of the open sets by repeated application of countable union, countable intersection, and complementation. An introduction in this direction is S. M. Srivastava’s &lt;a href=&quot;https://www.springer.com/gp/book/9780387984124&quot;&gt;&lt;em&gt;A Course on Borel Sets&lt;/em&gt;&lt;/a&gt;. I will also point to a set of accessible &lt;a href=&quot;http://www.personal.psu.edu/jsr25/Spring_11/574_Sp11_Syllabus.html&quot;&gt;lecture notes&lt;/a&gt; by Jan Reimann.&lt;/p&gt;

&lt;p&gt;This area forms the foundation for one of my primary interests: topological learning theory (link coming soon).&lt;/p&gt;

&lt;h3 id=&quot;history-of-measure-theory&quot;&gt;History of Measure Theory&lt;/h3&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block; margin: 0px 0px 0px 20px&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://www.maa.org/sites/default/files/HawkinsLebesque.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you’ve read Bressoud’s text that I offer as a supplement above, then you’ll be familiar with Thomas Hawkins’ &lt;a href=&quot;https://bookstore.ams.org/chel-282/&quot;&gt;&lt;em&gt;Lebesgue’s Theory of Integration: Its Origins and Developments&lt;/em&gt;&lt;/a&gt;. Hawkins’ book seems to be &lt;em&gt;the&lt;/em&gt; comprehensive account of the early history of the Lebesgue integral and the abstract measure theory that developed from it. I’ve not yet made it all the way through this one; it is on my list through!&lt;/p&gt;</content><author><name></name></author><summary type="html">When approaching measure theory for the first time, the ideas can seem opaque and unmotivated. This is amplified since many students of measure theory are not coming from a strictly mathematics background and may be approaching the material on their own outside of the classroom. In addition to first-year math graduate students and advanced math undergraduates, students in stats, economics, the hard sciences, etc. will find their way into learning measure theory. This is a guide to resources for learning measure theory that tries to keep in mind that many (myself included) approach the material with an atypical background.</summary></entry><entry><title type="html">Sets and Classes, Comprehension and Intuition</title><link href="http://localhost:4000/sets-classes-comprehension/" rel="alternate" type="text/html" title="Sets and Classes, Comprehension and Intuition" /><published>2019-07-05T00:00:00-05:00</published><updated>2019-07-05T00:00:00-05:00</updated><id>http://localhost:4000/sets-classes-comprehension</id><content type="html" xml:base="http://localhost:4000/sets-classes-comprehension/">&lt;p&gt;In many treatments of elementary set theory, classes are mentioned in passing or in the introductory pages in connection with Russell’s paradox. The moral is usually that not all classes are sets, but it’s not clear why we ought to care about classes in the first place. While reading &lt;a href=&quot;https://faculty.sites.uci.edu/pjmaddy/&quot;&gt;Penelope Maddy’s&lt;/a&gt; book, &lt;em&gt;Realism in Mathematics&lt;/em&gt;, I came across a brief but interesting discussion giving a historical perspective on sets and classes which sheds light on these concepts.&lt;/p&gt;

&lt;h2 id=&quot;sets-and-classes-two-perspectives-on-collections&quot;&gt;Sets and Classes: two perspectives on collections&lt;/h2&gt;

&lt;p&gt;Maddy’s discussion begins from the pre-theoretical notion of a collection. In the early 20th century, two competing perspectives emerged on how one ought to think about a collection of numbers or mathematical objects: one top-down and the other bottom-up. Maddy describes these as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“This [iteratively generated] idea of set was developed by Zermelo and became our modern iterative conception: sets are formed in hierarchy of stages. The contrasting idea of classes as extensions of properties, or better, of concepts, formed the groundwork of Frege’s theory. On this conception, we imagine the entire universe separated into two heaps depending on whether or not things have the given property or fall under the given concept. These contrasting ideas, set as iteratively generated versus class as extension, are often called the mathematical and logical notions of collection, respectively” Maddy (1983), 118-9.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The intuition for a class is that every class is collected by some property. One way to formalize this notion is to let $\phi (x)$ be a formula in some sufficiently powerful formal language $\mathcal{L}$. We may then separate the domain of discourse into the collection of objects satisfying the formula $\phi$, the extension of $\phi$, and those objects to do not satisfy $\phi$. We can then identify the extension of a formula with the class defined by that formula.&lt;/p&gt;

&lt;p&gt;Some classes seem very natural, e.g., the classes of primes, even integers, and algebraic real numbers. This perspective on collections has broad appeal, since some have held that only definable sets exist. For example, one objection to the Axiom of Choice put forward by Lebesgue and others is that the resulting choice set need not exist. (cite)&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;40%&quot; height=&quot;40%&quot; src=&quot;/images/blog/sets_classes/cumulative_hierarchy.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast, the iterative notion of a collection through ZFC and similar axiom systems begins by introducing the empty set $\emptyset$ and provides rules for constructing additional sets from known sets.&lt;/p&gt;

&lt;p&gt;[discuss cumulative hierarchy; rank]&lt;/p&gt;

&lt;p&gt;On this picture,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Sets are things which occur in the iterative hierarchy; they are formed in stages. Thus, the elements of a proper class like the class of all sets do all exist, but they do not exist together, in the sense that they do not form a set. The reason for this is now obvious: new sets are formed at each stage, so there cannot be a stage at which the set of all of them is formed. What is missing for this proper class is a stage after which all its elements have been formed.” Maddy (1983), 119.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;unlimited-comprehension&quot;&gt;Unlimited Comprehension&lt;/h2&gt;

&lt;p&gt;[what is unlimited comprehension and why is it reasonable]&lt;/p&gt;

&lt;p&gt;[Russel’s paradox]&lt;/p&gt;

&lt;p&gt;[Other attempts to resolve the paradox, e.g., classes can’t be members of other classes]&lt;/p&gt;

&lt;p&gt;[Restricted comprehension or separation; relation to the hierarchy]&lt;/p&gt;

&lt;p&gt;[Separation follows from existence and replacement]&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading:&lt;/h3&gt;

&lt;p&gt;Karel Hrbacek &amp;amp; Thomas Jech, &lt;em&gt;Introduction to Set Theory&lt;/em&gt;, 3rd Ed., 1999.&lt;/p&gt;

&lt;p&gt;Penelope Maddy, Proper Classes, &lt;em&gt;The Journal of Symbolic Logic&lt;/em&gt;, 48(1), 1983.&lt;/p&gt;

&lt;p&gt;Penelope Maddy, &lt;em&gt;Realism in Mathematics&lt;/em&gt;, 1990.&lt;/p&gt;</content><author><name></name></author><summary type="html">In many treatments of elementary set theory, classes are mentioned in passing or in the introductory pages in connection with Russell’s paradox. The moral is usually that not all classes are sets, but it’s not clear why we ought to care about classes in the first place. While reading Penelope Maddy’s book, Realism in Mathematics, I came across a brief but interesting discussion giving a historical perspective on sets and classes which sheds light on these concepts.</summary></entry><entry><title type="html">Parsing Nested JSON Records in Python</title><link href="http://localhost:4000/parsing-json-python/" rel="alternate" type="text/html" title="Parsing Nested JSON Records in Python" /><published>2019-06-22T00:00:00-05:00</published><updated>2019-06-22T00:00:00-05:00</updated><id>http://localhost:4000/parsing-json-python</id><content type="html" xml:base="http://localhost:4000/parsing-json-python/">&lt;p&gt;JSON is the typical format used by web services for message passing that’s also relatively human-readable. Despite being more human-readable than most alternatives, JSON objects can be quite complex. For analyzing complex JSON data in Python, there aren’t clear, general methods for extracting information (see &lt;a href=&quot;https://realpython.com/python-json/&quot;&gt;here&lt;/a&gt; for a tutorial of working with JSON data in Python). This post provides a solution if one knows the path through the nested JSON to the desired information.&lt;/p&gt;

&lt;h2 id=&quot;motivating-example&quot;&gt;Motivating Example&lt;/h2&gt;
&lt;p&gt;Suppose you have the following JSON record:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;employees&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Alice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Bob&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;firm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Charlie's Waffle Emporium&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;location&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CA&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This record has two keys at the top level: &lt;em&gt;employees&lt;/em&gt; and &lt;em&gt;firm&lt;/em&gt;. The value for the &lt;em&gt;employees&lt;/em&gt; key is a list of two objects of the same schema; each object has the keys &lt;em&gt;name&lt;/em&gt;, &lt;em&gt;role&lt;/em&gt;, and &lt;em&gt;nbr&lt;/em&gt;. The value for the &lt;em&gt;firm&lt;/em&gt; key is an object with the keys &lt;em&gt;name&lt;/em&gt; and &lt;em&gt;location&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Suppose you want to extract the names of the employees. This record will give problems for approaches that just search through key names, since the name of the firm will be returned as well.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;Calling the &lt;em&gt;extract_element_from_json&lt;/em&gt; function on the above record delivers the desired result:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;employees&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Alice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Bob&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;nbr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;firm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Charlie's Waffle Emporium&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;location&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;CA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;extract_element_from_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;employees&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Alice'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Bob'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;
&lt;p&gt;This function nests into the record(s) in &lt;em&gt;obj&lt;/em&gt; according to the keys specified in &lt;em&gt;path&lt;/em&gt; to retrieve the desired information. When a list is encountered as the value of a key in &lt;em&gt;path&lt;/em&gt;, this function splits and continues nesting on each element of the encountered list in a depth-first manner. This is how both ‘Alice’ and ‘Bob’ are returned; since the value of &lt;em&gt;employees&lt;/em&gt; is a list, the nesting is split on both of its elements and each of the values for &lt;em&gt;name&lt;/em&gt; are appended to the output list.&lt;/p&gt;

&lt;p&gt;If &lt;em&gt;obj&lt;/em&gt; is a single dictionary/JSON record, then this function returns a list containing the desired information, and if &lt;em&gt;obj&lt;/em&gt; is a list of dictionaries/JSON records, then this function returns a list of lists containing the desired information.&lt;/p&gt;

&lt;p&gt;If any element of &lt;em&gt;path&lt;/em&gt; is missing from the corresponding level of the nested dictionary/JSON, then this function returns a &lt;em&gt;None&lt;/em&gt; .&lt;/p&gt;

&lt;p&gt;Below is the full function (inspired/motivated from what’s discussed &lt;a href=&quot;https://hackersandslackers.com/extract-data-from-complex-json-python/&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;extract_element_from_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''
    Extracts an element from a nested dictionary or
    a list of nested dictionaries along a specified path.
    If the input is a dictionary, a list is returned.
    If the input is a list of dictionary, a list of lists is returned.
    obj - list or dict - input dictionary or list of dictionaries
    path - list - list of strings that form the path to the desired element
    '''&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'''
            Extracts an element from a nested dictionary
            along a specified path and returns a list.
            obj - dict - input dictionary
            path - list - list of strings that form the JSON path
            ind - int - starting index
            arr - list - output list
        '''&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outer_arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outer_arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outer_arr&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;update&quot;&gt;Update&lt;/h2&gt;
&lt;p&gt;This post is featured in Issue #374 of &lt;a href=&quot;https://pycoders.com/issues/374&quot;&gt;PyCoder’s Weekly&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">JSON is the typical format used by web services for message passing that’s also relatively human-readable. Despite being more human-readable than most alternatives, JSON objects can be quite complex. For analyzing complex JSON data in Python, there aren’t clear, general methods for extracting information (see here for a tutorial of working with JSON data in Python). This post provides a solution if one knows the path through the nested JSON to the desired information.</summary></entry><entry><title type="html">Introduction to Structure of Epistemic Justification via the Telephone Game (part I)</title><link href="http://localhost:4000/epistemic-justification/" rel="alternate" type="text/html" title="Introduction to Structure of Epistemic Justification via the Telephone Game (part I)" /><published>2019-03-09T00:00:00-06:00</published><updated>2019-03-09T00:00:00-06:00</updated><id>http://localhost:4000/epistemic-justification</id><content type="html" xml:base="http://localhost:4000/epistemic-justification/">&lt;p&gt;In epistemology, we often think of the things we believe as discrete propositions. For instance, you may believe that there is a computer screen in front of you. But how is this belief justified? One way of justifying a belief is by offering a reason, which can itself also be a proposition. For this next proposition, we can then ask how it is justified and so on. The regress problem asks the following question: if any of the things we believe are justified, then what is the structure of that justification? Does the justification question not just keep getting passed backward forever with reasons for reasons for reasons?&lt;/p&gt;

&lt;p&gt;To illustrate the common responses to the regress problem, imagine playing a game of telephone with a (potentially infinite) group of players. Telephone is played by secretly passing a message between players and comparing the message received at the end to messages received earlier in the game. Suppose you receive the message $M$ and wish to follow the message backward to compare the message you received to the original. You proceed by following the path of message passers backward and asking for the content of the message they had received. We can call the first message back $M_1$, the next message back $M_2$, and so forth.&lt;/p&gt;

&lt;p&gt;Below, we consider three common responses to the regress problem using this example. For simplicity, we will assume that a reason given to support a proposition is always a good reason and that a message is always passed from one player to the next iteratively; there are never two incoming messages at the same time. (We will drop this assumption in part II.) This results in a chain of messages of the following form: $M \leftarrow M_1 \leftarrow M_2 \leftarrow M_3 \leftarrow \cdots$.&lt;/p&gt;

&lt;h1 id=&quot;foundationalism&quot;&gt;Foundationalism&lt;/h1&gt;
&lt;p&gt;Suppose after tracing our message backward, the chain ends after $n$ steps at the original message $M_n$, much like with the usual game of telephone. We can compare the received message $M$ with original message $M_n$ to see how much the message was distorted through the passing chain. If $M$ and $M_n$ contain similar information then the playing group succeeds; otherwise, they fail. Notice that unless the game could have stopped at any point the comparison that really matters is between $M$ and $M_n$ and not the comparisons with intermediate messages.&lt;/p&gt;

&lt;p&gt;Similarly, for the foundationalist, the ultimate justification for our beliefs rests on these basic or foundational beliefs. Just as $M_n$ is where the message originates, basic beliefs are in some sense self-justified. Candidates for these basic beliefs include sense perceptions and naive seemings. For example, I am experiencing an itch or seeing a red triangle, or it seems that I have hands. The primary difficulty with this position is to give a clear account of basic beliefs: what they are and what are their epistemological properties. Foundationalism has been the dominant response to the regress problem until the second half of the 20th century, when coherentism gained popularity.&lt;/p&gt;

&lt;h1 id=&quot;coherentism&quot;&gt;Coherentism&lt;/h1&gt;
&lt;p&gt;Back in the game of telephone, imagine that after $n$ steps we reach a message that we’ve seen before. In this odd situation, we appear to be stuck in a loop as tracing the message onward would lead us to traverse the same sequence of messages again and again. How might we go about judging if the game has been played successfully or not? One method is to compare $M$ with all messages in the loop to see if they jointly contain similar information. Success results only in the case where all of the selected messages contain similar information.&lt;/p&gt;

&lt;p&gt;With respect to the justification of beliefs, the coherentist argues that justification emerges from the structure of a system of beliefs rather than ultimately resting on beliefs that are in some way self-justified. Consider a case of how this may work in practice, where we receive independent testimony of several witnesses to an event. Individually, we may not have a reason to believe the content of their testimony; however, if each witness reports a similar sequence of events, then belief in their individual reports may be more justified. We can say that these testimonies mutually support one another. On this account, this mutual support or coherence is what drives the justification of our beliefs.&lt;/p&gt;

&lt;p&gt;Many approaches in the contemporary literature combine elements from coherentism with a weak form of foundationalism, taking the best of both approaches.&lt;/p&gt;

&lt;h1 id=&quot;infinitism&quot;&gt;Infinitism&lt;/h1&gt;
&lt;p&gt;In this twist on game of telephone, suppose tracing the message backward is never-ending and we never repeat messages. This variant of telephone requires either infinitely many players or, at least, infinitely many potential players. As with coherentism, we find ourselves in a situation where judging success is unclear. One method is to compare $M$ to $M_n$ as $n$ grows ever larger. Suppose that each time we are prompted to judge the success of the game we incrementally check a larger $n$ and find that $M$ and $M_n$ contain similar information. In that case, we can say, at least provisionally, that the game has been played successfully.&lt;/p&gt;

&lt;p&gt;In contrast to foundationalism and coherentism, the infinitist holds that the infinite regress is not problematic. One popular position defending infinitism argues that to be justified in a belief one need only be able to give an additional reason when prompted, since no finite or human-like agent can offer infinitely many beliefs, as with the infinite variant of the telephone game above. Infinitism is not without its difficulties. At the least, one needs to give an account of what it means to both have access to and always be able to give another reason.&lt;/p&gt;

&lt;p&gt;Arguments for this brand of infinitism were introduced by Peter Klein and others in the late 20th Century.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The regress problem is far from settled. In the nearly two thousand years since its introduction, great strides have been made in understanding the structure of justification for our beliefs. Currently, some form of foundationalism is the dominant position, coherentism still has its defenders but has been in recent decline, and infinitism has started to be considered as a serious position during the last twenty years. Additionally, over the past decade, a new mathematical and probabilistic approach has emerged to recast the regress problem in more formal and interdisciplinary terms.&lt;/p&gt;

&lt;p&gt;In Part II of this article (coming soon), we will extend the analogy of the telephone game to allow for more complex forms of message passing. In turn, this will allow for a more robust characteriztion of the structure of epistemic justification, especially for both both finite and infinite varieties of coherentism.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;The material below is open access and available to all.&lt;/p&gt;

&lt;p&gt;On foundationalism for justification in epistemology:&lt;br /&gt;
Ali Hasan and Richard Fumerton. “Foundationalist Theories of Epistemic Justification”, The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), Edward N. Zalta (ed.), &lt;a href=&quot;https://plato.stanford.edu/archives/win2016/entries/justep-foundational&quot;&gt;https://plato.stanford.edu/archives/win2016/entries/justep-foundational&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On coherentism for justification in epistemology:&lt;br /&gt;
Erik Olsson. “Coherentist Theories of Epistemic Justification”, The Stanford Encyclopedia of Philosophy (Spring 2017 Edition), Edward N. Zalta (ed.), &lt;a href=&quot;https://plato.stanford.edu/archives/spr2017/entries/justep-coherence/&quot;&gt;https://plato.stanford.edu/archives/spr2017/entries/justep-coherence/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On infinitism for justification in epistemology:&lt;br /&gt;
Peter Klein and John Turri. “Infinitism in Epistemology”, Internet Encyclopedia of Philosophy, &lt;a href=&quot;https://www.iep.utm.edu/inf-epis/&quot;&gt;https://www.iep.utm.edu/inf-epis/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On the general form of regress arguments in philosophy:&lt;br /&gt;
Cameron Ross. “Infinite Regress Arguments”, The Stanford Encyclopedia of Philosophy (Fall 2018 Edition), Edward N. Zalta (ed.), &lt;a href=&quot;https://plato.stanford.edu/archives/fall2018/entries/infinite-regress/&quot;&gt;https://plato.stanford.edu/archives/fall2018/entries/infinite-regress/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On the mathematical approach to the regress problem:&lt;br /&gt;
David Atkinson and Jeanne Peijnenburg. 2017. Fading Foundations: Probability and the Regress Problem. Springer Open. &lt;a href=&quot;https://doi.org/10.1007/978-3-319-58295-5&quot;&gt;https://doi.org/10.1007/978-3-319-58295-5&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">In epistemology, we often think of the things we believe as discrete propositions. For instance, you may believe that there is a computer screen in front of you. But how is this belief justified? One way of justifying a belief is by offering a reason, which can itself also be a proposition. For this next proposition, we can then ask how it is justified and so on. The regress problem asks the following question: if any of the things we believe are justified, then what is the structure of that justification? Does the justification question not just keep getting passed backward forever with reasons for reasons for reasons?</summary></entry><entry><title type="html">Borel Hierarchy in Latex with TikZ</title><link href="http://localhost:4000/borel-hierarchy/" rel="alternate" type="text/html" title="Borel Hierarchy in Latex with TikZ" /><published>2019-01-12T00:00:00-06:00</published><updated>2019-01-12T00:00:00-06:00</updated><id>http://localhost:4000/borel-hierarchy</id><content type="html" xml:base="http://localhost:4000/borel-hierarchy/">&lt;p&gt;&lt;img width=&quot;40%&quot; height=&quot;40%&quot; src=&quot;/images/blog/borel_hierarchy/borel_hierarchy.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was looking online for a simple vertical Borel Hierarchy in Latex for a paper that I’m writing and had no luck. So I’ve written one. See the code below to use it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;k&quot;&gt;\documentclass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;article&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;tikz&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;\begin{document}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{figure}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\centering&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;\begin{tikzpicture}&lt;/span&gt; [scale=1]
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (delta01) at (0,0) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (sigma01) at (1,1) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (pi01) at (-1,1) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (delta02) at (0,2) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (sigma02) at (1,3) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (pi02) at (-1,3) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (deltamid) at (0,4) &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (sigmamid) at (1,5) &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (pimid) at (-1,5) &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (delta0alpha) at (0,4) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (sigma0alpha) at (1,5) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (pi0alpha) at (-1,5) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (delta0alpha1) at (0, 6) &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\Delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\alpha&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (deltatop) at (0,7) &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (sigmatop) at (1,7) &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (pitop) at (-1,7) &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta01) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (sigma01);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta01) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (pi01);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta02) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (sigma01);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta02) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (pi01);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta02) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (sigma02);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta02) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (pi02);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta02) -- (deltamid) node [midway, sloped] &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\dots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (sigma02) -- (sigmamid) node [midway, sloped] &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\dots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (pi02) -- (pimid) node [midway, sloped] &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\dots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta0alpha) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (sigma0alpha);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta0alpha) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (pi0alpha);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta0alpha1) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (sigma0alpha);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta0alpha1) edge node[left] &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (pi0alpha);
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (delta0alpha1) -- (deltatop) node [midway, sloped] &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\dots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (sigma0alpha) -- (sigmatop) node [midway, sloped] &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\dots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
    &lt;span class=&quot;k&quot;&gt;\path&lt;/span&gt; (pi0alpha) -- (pitop) node [midway, sloped] &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\dots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;;
  &lt;span class=&quot;nt&quot;&gt;\end{tikzpicture}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\caption&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Borel Hierarchy&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;g:borel&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;hierarchy&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{figure}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{document}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Top Books I’ve Read in 2018</title><link href="http://localhost:4000/Top-Books-2018/" rel="alternate" type="text/html" title="Top Books I've Read in 2018" /><published>2018-12-16T00:00:00-06:00</published><updated>2018-12-16T00:00:00-06:00</updated><id>http://localhost:4000/Top-Books-2018</id><content type="html" xml:base="http://localhost:4000/Top-Books-2018/">&lt;p&gt;Here are the top three books I’ve read in 2018. They are presented below in chronological order. While these three books seem rather disparate, they are bound together by themes of innovation, conflict, and ideology.&lt;/p&gt;

&lt;h2 id=&quot;science-and-change-1500-to-1700&quot;&gt;Science and Change: 1500 to 1700&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Hugh Kearney&lt;/p&gt;

&lt;p&gt;Published: 1971&lt;/p&gt;

&lt;p&gt;This book argues that the story of science’s emergence from the darkness during the Renaissance is an instance of revisionist history. &lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;20%&quot; height=&quot;20%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/3/31/Boyle_air_pump.jpg&quot; /&gt; This story is usually centered on Bacon, the Puritan spirit, the rise of Capitalism, experimentalism, and the founding of the Royal Society in England. In contrast, the author distinguishes between three dominant movements of thought between 1500 to 1700. The Organic movement dominated the beginning of the period having the authority of both church and state. This movement’s intellectual origins are found in Aristotle and biological analogies. The Mystical movement was a reaction to the Organic movement rooted in neo-platonism and alchemy. Finally, we have the usual protagonist in this story: the mechanist movement.&lt;/p&gt;

&lt;p&gt;The usual account holds that the mechanistic approach won out because the mechanists were also the experimentalists. With regard to experiments, differing motivations between practitioners in each tradition led to differences in the objects of experiments. &lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;30%&quot; height=&quot;30%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/William_Harvey_%281578-1657%29_Venenbild.jpg/1024px-William_Harvey_%281578-1657%29_Venenbild.jpg&quot; /&gt; For instance, the Organic approach experimented with blood flow and the purpose of the heart. We can see this illustrated in inventions as well: the Mystic approach made progress in the construction of furnaces, while the mechanists created better clockwork technology. So practitioners in all traditions experimented and invented; it was simply their motivations that differed.&lt;/p&gt;

&lt;p&gt;This book treads familiar ground of philosophy of science and general history of science. I had known that both Newton and Bacon was not the mechanists of their legacies. With that being said, I was unaware of the influence of neo-platonism and the Magical/Occult tradition during the Renaissance. This is a topic I intend to dive deeper into given my interests in Orphism and the so-called mystery cults of antiquity (an interest stemming from discussions of Dionysus and Bacchus in my all time favorite book &lt;a href=&quot;https://en.wikipedia.org/wiki/A_History_of_Western_Philosophy&quot;&gt;&lt;em&gt;A History of Western Philosophy&lt;/em&gt;&lt;/a&gt; by Bertrand Russell).&lt;/p&gt;

&lt;p&gt;The World Scientific printing of this book contains dozens of beautiful, glossy illustrations such as the figure above from William Harvey’s experiments on the circulation of blood. That alone is worth picking up a copy of this one.&lt;/p&gt;

&lt;h2 id=&quot;attack-of-the-50-foot-blockchain&quot;&gt;Attack of the 50 Foot Blockchain&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;https://rocknerd.co.uk/wp-content/uploads/2017/07/botandguy.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Author: David Gerard&lt;/p&gt;

&lt;p&gt;Published: 2017&lt;/p&gt;

&lt;p&gt;This book discusses both the origins and prospects of Bitcoin and other cryptocurrencies and blockchain as a product. Rather than being a dry, technical account of the software behind the scenes, Gerard focuses on the actual implementations and the ideologies of those involved. The writing style is comical and conversational, and the author pulls no punches in throwing cold water on the cryptocurrency hype. The two quotes below are illustrative:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Bitcoin community is now sufficiently dysfunctional that even such a simple proposal as “OK, let’s increase the block size to two megabytes” led to community schisms, code forks, retributive DDOS attacks, death threats, a split between Chinese miners and American core programmers … and plenty of other clear evidence that this and other problems in the Bitcoin protocol will never be fixed by a consensus process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Bitcoins are so hard for normal people to use that from CryptoLocker on, ransomware operators have been known to provide technical support to victims, so they can work out how to pay them and unlock their files. F-Secure even compiled a customer service evaluation of ransomware gangs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Gerard’s take is that cryptocurrencies have not delivered on their promises. The implementations of Bitcoin and Ethereum have been fraught with troubles, e.g., &lt;a href=&quot;https://www.wired.com/2016/06/50-million-hack-just-showed-dao-human/&quot;&gt;The DAO hack&lt;/a&gt;. The technologies are not easy to use, and the primary use case seems to be some form of money laundering. As for blockchain, there have been successful distributed ledgers, e.g. Git (which I use in conjunction with GitHub to host this site); however, for most use cases, it is overengineering.&lt;/p&gt;

&lt;p&gt;I picked up this book after three months into seriously learning about and eventually mining cryptocurrencies. Keep in mind that this was in the heat of the Bitcoin fever in December 2017! Outside of an &lt;a href=&quot;https://hackernoon.com/ten-years-in-nobody-has-come-up-with-a-use-case-for-blockchain-ee98c180100&quot;&gt;article on Hackernoon&lt;/a&gt;, many of the resources I came across were highly optimistic, e.g., &lt;a href=&quot;https://www.coursera.org/learn/cryptocurrency/home/info&quot;&gt;the &lt;em&gt;Bitcoin and Cryptocurrency Technologies&lt;/em&gt; Coursera course&lt;/a&gt;. Gerard’s book lent a good bit of perspective to my enthusiasm. I look forward to looking back at this one in a decade or so.&lt;/p&gt;

&lt;p&gt;You can read more on &lt;a href=&quot;https://davidgerard.co.uk/blockchain/&quot;&gt;David Gerard’s blog&lt;/a&gt; of the same name.&lt;/p&gt;

&lt;h2 id=&quot;exact-thinking-in-demented-times&quot;&gt;Exact Thinking in Demented Times&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Karl Sigmund&lt;/p&gt;

&lt;p&gt;Published: 2017&lt;/p&gt;

&lt;p&gt;This book chronicles the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vienna_Circle&quot;&gt;Vienna Circle&lt;/a&gt; (or Wiener Kreis), a motley collection of mathematicians, philosophers, and scientists who regularly met in interwar Vienna. The aim of the circle was to orient philosophy or foundations on a scientific basis to reflect developments in mathematics, logic, and the natural sciences and expunge most of traditional metaphysics and moral philosophy, a view referred to as logical positivism.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;30%&quot; height=&quot;30%&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/1/17/Schlick_sitting.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rather than focusing primarily on the evolution of logical positivism as an idea, Sigmund focuses on the lives and relations between members of the circle. While this account can come off as a bit gossipy, it is instructive in understanding the evolution of an exciting new research program composed of independent thinkers with political differences and big personalities. The Circle was chaired by Moritz Schlick (pictured to the right), a professor of inductive sciences at the University of Vienna. Other members of the circle featured include Rudolph Carnap, Otto Neurath, Phillip Frank, Viktor Kraft, and Hans Hahn.&lt;/p&gt;

&lt;p&gt;The author captures the excitement of the times through detailing the dissemination of the Circle’s research through a series of international conferences, a controversial manifesto (both internally and externally) called &lt;em&gt;The Scientific Conception of the World: The Vienna Circle&lt;/em&gt; (&lt;em&gt;Wissenschafteliche Weltauffassung: Der Wiener Kreis&lt;/em&gt;), an edited journal &lt;a href=&quot;https://www.springer.com/philosophy/journal/10670/PS2&quot;&gt;&lt;em&gt;Erkenntnis&lt;/em&gt;&lt;/a&gt; which is active today, and several published monographs.&lt;/p&gt;

&lt;p&gt;This excitement is best captured in A.J. Ayer’s 1936 report on the developments by the Vienna Circle from a visit in 1933: &lt;a href=&quot;https://en.wikipedia.org/wiki/Language,_Truth,_and_Logic&quot;&gt;Language, Truth, &amp;amp; Logic&lt;/a&gt;. My introduction to logical positivism was through Ayer’s book in a history of analytic philosophy seminar, and my continued interest in the Vienna Circle stems from their ability to have made influential contributions across many disciplines (particularly those I’m interested in) often regarded as requiring high specialization.&lt;/p&gt;

&lt;p&gt;I recommend this book as a companion to &lt;a href=&quot;https://www.harpercollins.com/9780060936648/wittgensteins-poker&quot;&gt;Wittgenstein’s Poker&lt;/a&gt;, the story of the encounter between two thinkers, Karl Popper and Ludwig Wittgenstein, each at the periphery of the Vienna Circle.&lt;/p&gt;</content><author><name></name></author><summary type="html">Here are the top three books I’ve read in 2018. They are presented below in chronological order. While these three books seem rather disparate, they are bound together by themes of innovation, conflict, and ideology.</summary></entry><entry><title type="html">Top Articles I’ve Read in 2018</title><link href="http://localhost:4000/Top-Articles-2018/" rel="alternate" type="text/html" title="Top Articles I've Read in 2018" /><published>2018-12-16T00:00:00-06:00</published><updated>2018-12-16T00:00:00-06:00</updated><id>http://localhost:4000/Top-Articles-2018</id><content type="html" xml:base="http://localhost:4000/Top-Articles-2018/">&lt;p&gt;Here are the top eleven papers I’ve come across in 2018.$^*$ These papers are mostly recent publications (within the last two years) with some older ones peppered in. They are in chronological order below.&lt;/p&gt;

&lt;h2 id=&quot;pick-the-largest-number&quot;&gt;Pick the Largest Number&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Thomas Cover&lt;/p&gt;

&lt;p&gt;Publication: Open Problems in Communication and Computation&lt;/p&gt;

&lt;p&gt;Published: 1987&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1007/978-1-4612-4808-8&quot;&gt;10.1007/978-1-4612-4808-8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This short note describes a solution to the two-card variant of a game called Googol. In this variant of the game, Player 1 writes down two distinct rational numbers $x_1, x_2$ and Player 2 guesses which of the two is larger after seeing the first number $x_1$. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_indifference&quot;&gt;principle of indifference&lt;/a&gt; gives a probability of success of one half by random guessing. The paper offers a strictly better solution. Pick a threshold $T$ from density $f(t)$ where $f(t)$ has support on the reals, e.g., the standard normal distribution. If $x_1 &amp;lt; T$, then say $x_2$ is larger and vice versa. Since there is a non-zero probability that $T$ is between $x_1$ and $x_2$, this strategy strictly dominates random guessing.&lt;/p&gt;

&lt;p&gt;On a first reading, this is an unexpected result that took a bit to wrap my mind around. Note that this conclusion does not violate the principle of indifference since Player 2 is not in a state of ignorance; they have observed the value of $x_1$. What makes this problem interesting is that $x_2$ does not depend on the value of $x_1$ so knowing $x_1$ seems irrelevant to determining the size of $x_2$.&lt;/p&gt;

&lt;p&gt;I came across this paper by way of the &lt;a href=&quot;https://fermatslibrary.com/journal_club&quot;&gt;Fermat’s Library Journal Club&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-trouble-with-psychological-darwinism&quot;&gt;The Trouble with Psychological Darwinism&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://static01.nyt.com/images/2017/12/01/arts/01fodor-obit-1/01fodor-obit-1-superJumbo.jpg?quality=90&amp;amp;auto=webp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Author: Jerry Fodor&lt;/p&gt;

&lt;p&gt;Publication: London Review of Books&lt;/p&gt;

&lt;p&gt;Published: Jan 1998&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.lrb.co.uk/v20/n02/jerry-fodor/the-trouble-with-psychological-darwinism&quot;&gt;www.lrb.co.uk/v20/n02/jerry-fodor/the-trouble-with-psychological-darwinism&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jerry Fodor reviews “How the Mind Works” by Stephen Pinker and “Evolution in Mind” by Henry Plotkin. These thinkers fall in line with the New Rationalism view of the mind which holds that the mind functions from modular structures and is partly innate (or the structures are anyway). This is in contrast to the blank-slate, social constructionist, plastic empiricist view. Fodor lays out four principles of Pinker’s and Plotkin’s views and points out where they deviate from his own. Here are the four: 1. the mind is a computational system; 2. the mind is largely modular; 3. a lot of cognitive structure is innate; 4. a lot of mental structure is an evolutionary adaption. Fodor contests the fourth point by adding that 3. doesn’t imply 4.&lt;/p&gt;

&lt;p&gt;A wonderful read that I couldn’t endorse more. Fodor’s writing is both lucid and highly entertaining. Here is a passage characteristic of this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Psychological Darwinism is a kind of conspiracy theory; that is, it explains behaviour by imputing an interest (viz in the proliferation of the genome) that the agent of the behaviour does not acknowledge. When literal conspiracies are alleged, duplicity is generally part of the charge: ‘He wasn’t making confetti; he was shredding the evidence. He did X in aid of Y, and then he lied about his motive.’ But in the kind of conspiracy theories psychologists like best, the motive is supposed to be inaccessible even to the agent, who is thus perfectly sincere in denying the imputation. In the extreme case, it’s hardly even the agent to whom the motive is attributed. Freudian explanations provide a familiar example: What seemed to be merely Jones’s slip of the tongue was the unconscious expression of a libidinous impulse. But not Jones’s libidinous impulse, really; one that his Id had on his behalf. Likewise, for the psychological Darwinist: what seemed to be your, after all, unsurprising interest in your child’s well-being turns out to be your genes’ conspiracy to propagate themselves. Not your conspiracy, notice, but theirs.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nytimes.com/2017/11/30/obituaries/jerry-a-fodor-dead-philosopher-of-the-mind.html&quot;&gt;Jerry Fodor passed away in late 2017.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-mathematicians-lament&quot;&gt;A Mathematician’s Lament&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Paul Lockhart&lt;/p&gt;

&lt;p&gt;Published: 2002&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;http://mysite.science.uottawa.ca/mnewman/LockhartsLament.pdf&quot;&gt;http://mysite.science.uottawa.ca/mnewman/LockhartsLament.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This essay is an indictment of the modern K-12 math curriculum primarily because mathematics is presented as unmotivated, algorithmic drudgery by educators who have never done mathematics. This is a wonderful and hilarious read! The author argues that mathematics classes should be less structured, more argument based, and concepts and distinctions should be situated in their historical and philosophical context.&lt;/p&gt;

&lt;p&gt;The author runs with the analogy of painting where we would never teach painting without having the students actually paint. The proper analogue with the current mathematics curriculum would be having the students memorize colors, brushes, styles, and terse terminology with no motivation or context and not ever really have them paint except in some rigidly defined sense, e.g., painting by numbers or tracing from a stencil.&lt;/p&gt;

&lt;p&gt;The view is pushed that mathematics is an art whose practical consequences are a biproduct rather than an end. This is similar to those who argue that philosophy is an activity rather than a subject.&lt;/p&gt;

&lt;h2 id=&quot;to-explain-or-to-predict&quot;&gt;To Explain or to Predict&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Galit Shmueli&lt;/p&gt;

&lt;p&gt;Publication: Statistical Science&lt;/p&gt;

&lt;p&gt;Published: Aug 2010&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1214/10-STS330&quot;&gt;10.1214/10-STS330&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper covers the difference between prediction and explanation in depth. It discusses the difference in the modeling process from conception to model validation featuring two case studies with the Netflix Challenge data and auction data. The author distinguishes descriptive modeling from the tasks of prediction and explanation; however, descriptive modeling is not covered in depth.&lt;/p&gt;

&lt;p&gt;While this paper is not groundbreaking, it provides an excellent overview between different sorts of modeling especially for those coming from machine learning-heavy or statistics-heavy backgrounds. The appendix provides an example of when a properly specified model, i.e., the true model, may not be the most predictive. Though counterintuitive at first, this result demonstrates the fundamental difference between the tasks of prediction and explanation.&lt;/p&gt;

&lt;h2 id=&quot;progress-in-evidence-based-medicine-a-quarter-century-on&quot;&gt;Progress in evidence-based medicine: a quarter century on&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;https://s3.amazonaws.com/libapps/customers/4658/images/EBM.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Authors: Benjamin Djulbegovic &amp;amp; Gordon H Guyatt&lt;/p&gt;

&lt;p&gt;Publication: The Lancet&lt;/p&gt;

&lt;p&gt;Published: Feb 2017&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1016/S0140-6736(16)31592-6&quot;&gt;doi.org/10.1016/S0140-6736(16)31592-6&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article introduces Evidence Based Medicine (EBM) and it’s three main tenets: 1. not all evidence is created equal; 2. all relevant evidence should be used in decision making; 3. clinical decision making should consider patients’ values and preferences. Initially, the field pushed for practitioners to remain up-to-date on the latest research in their field; however, this is too tall of a task, so the movement changed emphasis to systematic studies and literature overviews.&lt;/p&gt;

&lt;p&gt;One prominent criticism addressed is that this leads to algorithmic decision making that is divorced from the human element. This is true is two respects: 1. the reasoning may be too complex for the patient to understand; 2. the individual practitioner’s influence is greatly reduced.&lt;/p&gt;

&lt;p&gt;I found this paper particularly interesting given that there’s an close analogy between EBM and data science that would have not been apparent prior to working in industry.&lt;/p&gt;

&lt;p&gt;This paper was featured in the EBM column of an issue of &lt;a href=&quot;http://www.thereasoner.org&quot;&gt;The Reasoner&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;understanding-deep-learning-requires-rethinking-generalization&quot;&gt;Understanding Deep Learning Requires Rethinking Generalization&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, &amp;amp; Oriol Vinyals&lt;/p&gt;

&lt;p&gt;Publication: arXiv&lt;/p&gt;

&lt;p&gt;Published: Feb 2017&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;arxiv.org/abs/1611.03530&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this paper, the authors present empirical and theoretical results on generalization error and the capacity for memorization in deep learning models, specifically with convolutional neural networks (CNN). CNN architectures exist that achieve near-perfect performance on standard image datasets, e.g. CIFAR10, with low generalization error. The authors found that by randomly permuting the image labels, the same architectures produce models with near-perfect accuracy on the training data but, correspondingly, a high generalization error. They summarize their finding as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Deep neural networks easily fit random labels.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To put this result into context, the authors explore how different regularization techniques, e.g., dropout, weight decay, etc., affect generalization error. The conclusion is along the lines of &lt;em&gt;we don’t know what causes neural networks to generalize well.&lt;/em&gt; I do want to emphasize that while I found this paper interesting, readable, and helpful to understanding deep learning models, it may have received more hype from the ML community than it deserves.&lt;/p&gt;

&lt;p&gt;I crossed paths with this paper while browsing &lt;a href=&quot;https://blog.acolyer.org&quot;&gt;the morning paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stuck-the-law-and-economics-of-residential-stability&quot;&gt;Stuck! The Law and Economics of Residential Stability&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: David Schleicher&lt;/p&gt;

&lt;p&gt;Publication: Yale Law Journal&lt;/p&gt;

&lt;p&gt;Published: October 2017&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://www.yalelawjournal.org/article/stuck-the-law-and-economics-of-residential-stagnation&quot;&gt;https://www.yalelawjournal.org/article/stuck-the-law-and-economics-of-residential-stagnation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article discusses the phenomena of declining interstate mobility and some of its potential causes and effects. To summarize the decline:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Americans are not leaving places hit by economic crises, resulting in unemployment rates and low wages that linger in these areas for decades. And people are not moving to rich regions where the highest wages are available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The author examines the state and local policies that affect interstate mobility both those that impede entering a new state, e.g., differential licensure requirements, and those that impede leaving one’s current state, e.g., encouraging homeownership through the mortgage interest deduction and other subsidies. One effect discussed extensively of the sluggish labor market resulting from decreased interstate mobility is the decreased effectiveness of federal macroeconomic policy. Consider two neighboring states where one state is booming while the other busts but there is little population movement between the states. In this case, there may not be a uniform monetary policy beneficial to both states.&lt;/p&gt;

&lt;p&gt;This article is interesting primarily as a dip of one’s toes into the depths of economic mobility.&lt;/p&gt;

&lt;p&gt;I came across this article in the Further Reading from the &lt;a href=&quot;https://www.aeaweb.org/journals/jep&quot;&gt;Journal of Economic Perspectives&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;top5itis&quot;&gt;TOP5ITIS&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Roberto Serrano&lt;/p&gt;

&lt;p&gt;Status: Working paper as of Jan 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;http://www.econ.brown.edu/Faculty/serrano/pdfs/wp2018-2-Top5itis.pdf&quot;&gt;www.econ.brown.edu/Faculty/serrano/pdfs/wp2018-2-Top5itis.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper is a commical commentary on both the academic economists’ infatuation with the &lt;em&gt;Top5&lt;/em&gt; economics journals and the recent discussions of them at the AEA meetings. The author charges the practice of focusing primairly on &lt;em&gt;Top5&lt;/em&gt; publications as arbitrary gatekeeping that potentially stifles innovation that is not clearly a good measure of either one’s performance as an academic or one’s expertise in a topic. A sample of the goodness:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Since the disease is a simple application of the counting measure (typically a person learns to count in primary school), through a process of contagion, top5itis spreads quickly to affect people outside economics, including schoolchildren offspring of economists who get together in the playground to make disparaging remarks about each other’s parents.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;learning-is-a-risky-business&quot;&gt;Learning is a Risky Business&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Author: Wayne C. Myrvold&lt;/p&gt;

&lt;p&gt;Publication: Erkenntnis&lt;/p&gt;

&lt;p&gt;Published: Feb 2018&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1007/s10670-018-9972-0&quot;&gt;10.1007/s10670-018-9972-0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper advances an argument against the &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_indifference&quot;&gt;principle of indifference&lt;/a&gt;. Roughly, the principle of indifference says that absent evidence to the contrary one should assign equal probability to each possible state of the world. The author’s argument proceeds by introducing time into the partitioning of the state space. Simply put: consider a year where each day either $\phi$ or $\neg \phi$. Then there are $2^{365}$ possible histories. The principle of indifference assigns equal probability to each; however, this implies that the assessment of probabilities on each day is independent of what happened on prior days, i.e., one does not learn from experience.&lt;/p&gt;

&lt;p&gt;It’s a compelling argument that may require the notion of &lt;em&gt;learn&lt;/em&gt; to be tightened up. Also: the paper’s leading example involves a SuperBaby.&lt;/p&gt;

&lt;h2 id=&quot;anchors-high-precision-model-agnostic-explanations&quot;&gt;Anchors: High-Precision Model-Agnostic Explanations&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img style=&quot;float: right; display: inline-block&quot; width=&quot;40%&quot; height=&quot;40%&quot; src=&quot;/images/blog/best_articles_2018/anchor.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Authors: Marco Túlio Ribeiro, Sameer Singh, &amp;amp; Carlos Guestrin&lt;/p&gt;

&lt;p&gt;Publication: Proceedings of The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)&lt;/p&gt;

&lt;p&gt;Published: Feb 2018&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982&quot;&gt;https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper introduces an approach to prediction explanation called anchors as a refinement of the LIME approach. With LIME, model behavior is assessed by sampling points around the observation of interest and building an interpretable model on the sampled data to find out which variables are driving the prediction. More on LIME &lt;a href=&quot;https://homes.cs.washington.edu/~marcotcr/blog/lime/&quot;&gt;here&lt;/a&gt;. One difficulty with building local models for post-hoc prediction is that it is not clear how far out from the observation of interest that the local model generalizes. This concern is addressed by introducing anchors, topologically simple regions of the feature space where a prediction holds with high probability. Think of anchors as a sufficiency condition. This method can better generalize because it is essentially claiming less. Note that at present the anchor approach only works with binary classification models.&lt;/p&gt;

&lt;p&gt;The GitHub repo to test out these techniques can be &lt;a href=&quot;https://github.com/marcotcr/anchor&quot;&gt;found here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;understanding-and-misunderstanding-randomized-controlled-trials&quot;&gt;Understanding and misunderstanding randomized controlled trials&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Authors: Angus Deaton and Nancy Cartwright&lt;/p&gt;

&lt;p&gt;Publication: Social Science &amp;amp; Medicine&lt;/p&gt;

&lt;p&gt;Published: Aug 2018&lt;/p&gt;

&lt;p&gt;DOI: &lt;a href=&quot;https://doi.org/10.1016/j.socscimed.2017.12.005&quot;&gt;10.1016/j.socscimed.2017.12.005&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article evaluates the benefits and limitations of randomized control trials (RCT). In practice, the goal of running an RCT is to obtain an estimate of the average effect from some treatment employing as few assumptions as possible; indeed, estimating the average treatment effect (ATE) is a nonparametric method. We often think of an RCT is the “gold-standard” evidence for the existence of a causal effect in medicine, economics, etc.; however, the effectiveness of an RCT comes from having no prior evidence. If we have more information about the underlying causal mechanism, then we can perform more accurate inference by stratifying our randomization by causal pathways, for instance, or employ various other causal modeling techniques. The value of RCTs is in when we do not have the same assumptions; however, once we agree on assumptions, we can leverage those assumptions to use other estimation methods.&lt;/p&gt;

&lt;p&gt;Applying the results of an RCT is tricky. Generalizing from a sample to a population is suspect in the social sciences due to all of the potential differences between groups, e.g., the sample may not have been random or there may be been post-randomization confounding, also called realized confounding. Often the estimated ATE is applied to the individual; however, this is entirely erroneous, since the ATE is an average and an individual’s characteristics will differ from the average.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“In these conversations, the results of an RCT may have marginal value. If your physician tells you that she endorses evidence-based medicine, and that the drug will work for you because an RCT has shown that ‘it works’, it is time to find a physician who knows that you and the average are not the same.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://causality.cs.ucla.edu/blog/index.php/2018/03/10/challenging-the-hegemony-of-randomized-controlled-trials-comments-on-deaton-and-cartwright/&quot;&gt;See here&lt;/a&gt; for comments on this paper from Judea Pearl.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;$^*$Since we’re likely not Pythagoreans, there’s probably not a good reason for preferring ten articles to eleven.&lt;/p&gt;</content><author><name></name></author><summary type="html">Here are the top eleven papers I’ve come across in 2018.$^*$ These papers are mostly recent publications (within the last two years) with some older ones peppered in. They are in chronological order below.</summary></entry></feed>